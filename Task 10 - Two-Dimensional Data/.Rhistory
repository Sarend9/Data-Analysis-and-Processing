#    • Определите, насколько хорошо результат
#      одного экзамена предсказывает другой, через R2.
# 2) Регрессионная модель
#    -/ОБЩАЯ ТЕОРИЯ/-
#    В рамках регрессионного анализа существует возможность
#    ПРЕДСКАЗЫВАТЬ значения одной из переменных по значениям другой, поэтому:
#    • переменную-“фактор”    - принято называть предиктором (predictor),
#    • “зависимую” переменную - принято называть откликом (response variable).
#    • При ДВУМЕРНЫХ данных модель называют «простой регрессией», в ней:
#      o  один предиктор - “фактор”;         ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#      o  один отклик    - “зависимая” переменная.
#    • При МНОГОМЕРНЫХ данных модель называют «множественной регрессией», в ней:
#      o  несколько переменых предикторов;     ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#    2.1) коэффициент детерминации R²
#         ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#         Модели «дисперсионного» и «регрессионного» анализа основаны на одном и том же
#         принципе разложения изменчивости зависимой переменной (отклика) на две составляющие:
#         • эффект, связанный с фактором (предиктором),
#         • и нормально распределённые остатки. Обе эти модели — разновидности т.н. общей линейной модели
#
#         Для вычисления меры связи переменных в модели линейной регрессии мы
#         можем воспользоваться тем, что в линейной модели общая дисперсия
#         переменной-отклика равна сумме дисперсий эффекта и остатков:
#         σ²_total = σ²_effect + σ²_residual
#         Соотвественно, мы можем вычислить η2 двумя способами:
#
#                 SS_effect        SS_residual
#         η2  =   —————————  = 1 - ———————————
#                 SS__total         SS__total
#
#         -/ВАЖНО/-
#         • Для «дисперсионного» способа - считать через SS_effect, т.к. есть факторы (есть группы)
#         • Для «регрессионного» способа - считать через SS_residual - сумма квадратов остатков
#         Т.к. в «регрессионном» анализе групп НЕТ, в нем мера связи называется
#         НЕ «коэффициент внутригрупповой корреляции η2», - а «коэффициент детерминации R²»
#                  SS_residual
#         R² = 1 - ———————————
#                   SS__total
#         -/Примечание/-
#         В случае ЛИНЕЙНОЙ регрессии коэф. R² можно вычилсть еще R²=p², где p - коэф. линейн. кор. Пирсона.
#         2.1.1) Для нахождения коэффициента корреляции R2
#                начнём с вычисления «групповых средних».
x3_R2 <- x3
x3_R2 <- data.frame("test"   = c(rep(names(x3[1]), length(x3[[1]])), rep(names(x3[2]), length(x3[[2]]))),
"result" = c(x3[[1]], x3[[2]])
)
x3_R2[[1]] <- factor(x3_R2[[1]]); str(x3_R2 )
#                Очистим данные от «NA»:
colSums(is.na(x3_R2))
x3_R2 <- na.omit(x3_R2)
colSums(is.na(x3_R2))
#                Средние по группам:
x3_R2.grp.mean <- tapply(x3_R2[[2]], x3_R2[[1]], FUN = mean)
x3_R2.grp.mean
#         2.1.2) Далее найдём отклонения «групповых средних» от «общего среднего»:
x3_R2.grp.dev <- (x3_R2.grp.mean  -  mean(x3_R2.grp.mean))
x3_R2.grp.dev
#         2.1.3) Теперь нам нужно сложить квадраты этих отклонений, посчитав
#                квадрат отклонения каждого группового среднего столько раз,
#                сколько в данной группе наблюдений. Другими словами, перед
#                суммированием каждое значение grp.dev, возведённое в квадрат,
#                нужно умножить на частоту соответствующего уровня фактора.
SS_effect <- sum((x3_R2.grp.dev^2)*table(x3_R2[[1]]))
SS_effect
#                Вычислим общую сумму квадратов отклонений:
SS_total <- sum((x3_R2[[2]] - mean(x3_R2.grp.mean))^2)
SS_total
SS_residual <- SS_total - SS_effect
SS_residual
#                Найдем частное двух сумм:
R2 <- 1 - (SS_residual/SS_total)
R2
P.Pearson <- function(x.table)
{
x.table <- na.omit(x.table)
sd_x    <- sqrt(sum((x.table[[1]] - mean(x.table[[1]]))^2)/nrow(x.table))
sd_y    <- sqrt(sum((x.table[[2]] - mean(x.table[[2]]))^2)/nrow(x.table))
Σ       <- sum((x.table[[1]]-mean(x.table[[1]]))*(x.table[[2]]-mean(x.table[[2]])))
N.σx.σy <- nrow(x.table)*sd_x*sd_y
return(Σ/N.σx.σy)
}
#                Ответ:
P <- P.Pearson(x3); P
R2 <- P^2; R2
#    2.2) коэффициенты регрессии
#         ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#         Нужно провести оптимальную линию регрессии через облако рассеяния.
#         Самый простой способ: линия выбранной формы накладывается на данные
#         (модель “подгоняется”) так, чтобы остатки (отклонения значений переменной-отклика)
#         от этой линии, оказались минимальными.
#         Точнее, минимальными должны быть квадраты остатков (сумма этих квадратов) —
#         такой способ подгонки модели называют «методом наименьших квадратов».
#                                                ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#         По сути дела, построить, или подогнать регрессионную модель:
#         найти её коэффициенты, β1 и β0 для простой линейной регрессии.
#
#                        | Коэффициент β1 определяет угол наклона линии регрессии.
#                σ_Y     | p - коэффициент линейной корреляции Пирсона.
#         β1 = p ———     |
#                σ_X     |
#                              | Коэффициент β0 — свободный член, константа, или интерцепт
#                              | представляет собой значение Yi при Xi = 0, т.е точку "оси y"
#         β0 = μ_Y - (β1*μ_X)  | в месте её пересечения (intercept) линией регрессии, фактически
#                              | величину сдвига всей линии регрессии по оси y относительно параллельной
#                              | прямой, проходящей через начало координат.
σ_X <- sqrt(sum((x3[[1]] - mean(x3[[1]]))^2)/length(x3[[1]]))
σ_Y <- sqrt(sum((x3[[2]] - mean(x3[[2]]))^2)/length(x3[[2]]))
β1  <- P*(σ_Y/σ_X)
β1
μ_X <- mean(x3[[1]])
μ_Y <- mean(x3[[2]])
β0 = μ_Y - (β1*μ_X)
β0
# СИНИЯ ЛИНИЯ
plot(x3, xlim = c(0,35), ylim = c(0,40))
abline(β0, β1, col = "steel blue")
# ФИОЛЕТОВАЯ ЛИНИЯ
β1.yx  <- P*(σ_X/σ_Y)
β1.inverse <- 1/β1.yx  # замена коэффициента регрессии на обратную величину
β0.inverse <- μ_Y - (β1.inverse*μ_X)  # вычисление свободного члена
abline(β0.inverse, β1.inverse,
col = 'maroon',
lty = 'dotted')  # пунктир
# ЗЕЛЕНАЯ ЛИНИЯ
β1.inverse <- 1/β1
β0.inverse <- μ_Y - (β1.inverse*μ_X)
abline(β0.inverse, β1.inverse,
col = 'green3',
lty = 'dotted')  # пунктир
β1.2 <- (σ_X/σ_Y)
β0.2 <- μ_Y - (β1.2*μ_X)
abline(β0.2, β1.2,
col = 'green3',
lty = 12)  # пунктир
# ФИОЛЕТОВАЯ ЛИНИЯ
β1.yx  <- P*(σ_X/σ_Y)
β1.inverse <- 1/β1.yx  # замена коэффициента регрессии на обратную величину
β0.inverse <- μ_Y - (β1.inverse*μ_X)  # вычисление свободного члена
abline(β0.inverse, β1.inverse,
col = 'maroon',
lty = 'dotted')  # пунктир
# СИНИЯ ЛИНИЯ
plot(x3, xlim = c(0,35), ylim = c(0,40))
abline(β0, β1, col = "steel blue")
# ФИОЛЕТОВАЯ ЛИНИЯ
β1.yx  <- P*(σ_X/σ_Y)
β1.inverse <- 1/β1.yx  # замена коэффициента регрессии на обратную величину
β0.inverse <- μ_Y - (β1.inverse*μ_X)  # вычисление свободного члена
abline(β0.inverse, β1.inverse,
col = 'maroon',
lty = 'dotted')  # пунктир
# ЗЕЛЕНАЯ ЛИНИЯ
β1.inverse <- 1/β1
β0.inverse <- μ_Y - (β1.inverse*μ_X)
abline(β0.inverse, β1.inverse,
col = 'green3',
lty = 'dotted')  # пунктир
x123[[1]] <- relevel(x123[[1]], ref = "OGE")
regmodel <- lm(x3[[1]] ~ x3[[2]], data = x3)
summary(regmodel)
summary(aov(regmodel))
summary(aov(lm(x3[[1]] ~ x3[[2]], data = x3)))
a<-function(x){
summary(aov(lm(x[[2]] ~ x[[1]], data = x))
}
a<-function(x){
summary(aov(lm(x[[2]] ~ x[[1]], data = x)))
}
b<-function(x){
summary(aov(lm(x[[1]] ~ x[[2]], data = x)))
}
a(x1)
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Задание 10/Вопрос 1-2 (10).R")
a(x1)
a(x3)
b(x1)
b(x2)
b(x3)
b(x1)
634.6/(634.6+466.2)
x1
x1[1]==A
x1[[1]]==A
x1[[1]]=="A"
x1[x1[[1]]=="A",1]
data.frame("A" = x1[x1[[1]]=="A",1], "B" = x1[x1[[1]]=="B",1], "C" = x1[x1[[1]]=="C")
data.frame("A" = x1[x1[[1]]=="A",1], "B" = x1[x1[[1]]=="B",1], "C" = x1[x1[[1]]=="C" )
data.frame("A" = x1[x1[[1]]=="A",1], "B" = x1[x1[[1]]=="B",1], "C" = x1[x1[[1]],1]=="C" )
x123 <- data.frame("test"   = c(rep(names(x3[1]), length(x3[[1]])), rep(names(x3[2]), length(x3[[2]]))),
"result" = c(x3[[1]], x3[[2]])
)
x123[[1]] <- factor(x123[[1]]); str(x123)
x123
a(x1)
a(x123)
View(x3)
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.func(x123)
View(x3)
x22 <- unique(x3)
View(x22)
x123 <- test.func(x22)
test.func <- function(x){
x <- data.frame("test"   = c(rep(names(x[1]), length(x[[1]])), rep(names(x[2]), length(x[[2]]))),
"result" = c(x[[1]], x[[2]]))
return(x)
}
x123 <- test.func(x22)
a(x1)
a(x22)
a(x123)
b(x1)
b(x2)
b(x3)
lm(x3[[1]] ~ x3[[2]], data = x3)
summary(lm(x3[[1]] ~ x3[[2]], data = x3))
summary((x3[[1]] ~ x3[[2]], data = x3))
((x3[[1]] ~ x3[[2]], data = x3))
1 ~ 1
1 ~ 1 +3
1 ~ 1 + 3
1 + 3
x3[[1]] ~ x3[[2]]
summary(lm(x3[[1]], [[2]], data = x3))
summary(lm(x3[[1]] ~ [[2]], data = x3))
summary(lm(x3[[1]] ~ x3[[2]], data = x3))
summary(lm(x3[[1]], x3[[2]], data = x3))
b(x3)
634.6 + 466.2
sum((x3[[2]] - mean(x3_R2[[2]]))^2)
sum((x3[[2]] - mean(x3_R2[[2]]))^2)/nrow(x3[[2]])
sum((x3[[2]] - mean(x3_R2[[2]]))^2)/nrow(x3[2])
sum((x3[[2]] - mean(x3_R2[[2]]))^2)/nrow(x3)
sum((x3[[2]] - mean(x3[[2]]))^2)/nrow(x3[2])
x3[[2]]
x3[[1]]
a<-b(x3)
a
a[[1]]
as.data.frame(a)
t(a)
t(a)
anova(a)
b<-function(x){
(lm(x[[1]] ~ x[[2]], data = x)))
b<-function(x){
(lm(x[[1]] ~ x[[2]], data = x))
}
a<-b(x3)
a<-anova(a)
a
a<-b(x3)
a<-anova(a)
a<-b(x3)
View(a)
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.func(x1)
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- lm(x[[1]] ~ x[[2]], data = x);
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M)
M <- t(M); M <- as.data.frame(M); names(M) <- c("Factor", "Error"); M <- t(M)
print(M)}
x3<-x.anova(x3)
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M)
M <- t(M); M <- as.data.frame(M); names(M) <- c("Factor", "Error"); M <- t(M)
print(M)}
x3.anova<-x.anova(x3)
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M)
M <- t(M); M <- as.data.frame(M); names(M) <- c("Factor", "Error"); M <- t(M)
print(M)}
x3.anova<-x.anova(x3)
# Вопрос 3: Имеются результаты ОГЭ и ЕГЭ группы респондентов: exams.dat.
#
#           Постройте для этой группы модель регрессии балла ЕГЭ на
#           балл ОГЭ, исходя из линейности связи двух этих показателей.
x3 <- read.csv("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Задание 10/3_4.exams.dat")
# -/МОРАЛЬ/-
# 1) "Проверь какие данные в переменной"
# 2) "Смотри сначала на диаграмму, прежде чем считать коэффициенты"
# 1) Проверим данные:
#    1.1) Посмотрим в консоли что из себя предсатвяют данные:
print("начальные строки объекта:");     head(x3)
print("внутренняя структура объекта:"); str(x3)
#    1.2) Посмотрим график:
plot(x3)
#        -/ВЫВОД/-
#        В отличии от задания с дисперсионным анализом, здесь
#        обе переменные метрические, но мера связи по прежнему
#        будет служить коэффициент  η2 — с точки зрения вычислений
#        принципиально ничего не изменится, только метод будет называться
#        уже не дисперсионным, а регрессионным анализом.
#    Задачи:
#    • Введите значение β1;
#    • Введите значение β0;
#    • Определите, насколько хорошо результат
#      одного экзамена предсказывает другой, через R2.
# 2) Регрессионная модель
#    -/ОБЩАЯ ТЕОРИЯ/-
#    В рамках регрессионного анализа существует возможность
#    ПРЕДСКАЗЫВАТЬ значения одной из переменных по значениям другой, поэтому:
#    • переменную-“фактор”    - принято называть предиктором (predictor),
#    • “зависимую” переменную - принято называть откликом (response variable).
#    • При ДВУМЕРНЫХ данных модель называют «простой регрессией», в ней:
#      o  один предиктор - “фактор”;         ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#      o  один отклик    - “зависимая” переменная.
#    • При МНОГОМЕРНЫХ данных модель называют «множественной регрессией», в ней:
#      o  несколько переменых предикторов;     ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#    2.1) коэффициент детерминации R²
#         ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#         Модели «дисперсионного» и «регрессионного» анализа основаны на одном и том же
#         принципе разложения изменчивости зависимой переменной (отклика) на две составляющие:
#         • эффект, связанный с фактором (предиктором),
#         • и нормально распределённые остатки. Обе эти модели — разновидности т.н. общей линейной модели
#
#         Для вычисления меры связи переменных в модели линейной регрессии мы
#         можем воспользоваться тем, что в линейной модели общая дисперсия
#         переменной-отклика равна сумме дисперсий эффекта и остатков:
#         σ²_total = σ²_effect + σ²_residual
#         Соотвественно, мы можем вычислить η2 двумя способами:
#
#                 SS_effect        SS_residual
#         η2  =   —————————  = 1 - ———————————
#                 SS__total         SS__total
#
#         -/ВАЖНО/-
#         • Для «дисперсионного» способа - считать через SS_effect, т.к. есть факторы (есть группы)
#         • Для «регрессионного» способа - считать через SS_residual - сумма квадратов остатков
#         Т.к. в «регрессионном» анализе групп НЕТ, в нем мера связи называется
#         НЕ «коэффициент внутригрупповой корреляции η2», - а «коэффициент детерминации R²»
#                  SS_residual
#         R² = 1 - ———————————
#                   SS__total
#         -/Примечание/-
#         В случае ЛИНЕЙНОЙ регрессии коэф. R² можно вычилсть еще R²=p², где p - коэф. линейн. кор. Пирсона.
#         2.1.1) Через встроенную функцию найдем суммы квадратов:
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M)
M <- t(M); M <- as.data.frame(M); names(M) <- c("Factor", "Error"); M <- t(M)
print(M)}
x3.anova<-x.anova(x3)
x3.anova
View(x3.anova)
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M)
M <- t(M); M <- as.data.frame(M); names(M) <- c("Factor", "Error", "Total"); M <- t(M)
print(M)}
x3.anova<-x.anova(x3)
x3.anova<-x.anova(x3); x3.anova
x3.anova
x<-x3
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M)
M <- t(M);
M <- as.data.frame(M);
M
M[[2]]
M[[3]] <- c(sum())
M
M[1,]
M[1,1]
M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA)
M
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M)
M <- t(M);
M <- as.data.frame(M);
M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA)
names(M) <- c("Factor", "Error", "Total"); M <- t(M)
print(M)}
x3.anova<-x.anova(x3); x3.anova
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M)
M <- t(M);
M <- as.data.frame(M);
M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA)
names(M) <- c("Factor", "Error", "Total"); M <- t(M)}
x3.anova<-x.anova(x3); x3.anova
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M); M <- t(M); M <- as.data.frame(M);
M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA)
names(M) <- c("Factor", "Error", "Total"); M <- t(M)}
x3.anova<-x.anova(x3); x3.anova
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.func(x1)
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M); M <- t(M); M <- as.data.frame(M);
M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA)
names(M) <- c("Factor", "Error", "Total"); M <- t(M)}
x3.anova<-x.anova(x3); x3.anova
x3.anova
x3.anova[[2]]
x3.anova[[3]]
x3.anova[1]
x3.anova[2]
x3.anova[3]
x3.anova[4]
x3.anova[5]
R2 <- 1 - (SS_residual/SS_total);  R2
P.Pearson <- function(x.table)
{
x.table <- na.omit(x.table)
sd_x    <- sqrt(sum((x.table[[1]] - mean(x.table[[1]]))^2)/nrow(x.table))
sd_y    <- sqrt(sum((x.table[[2]] - mean(x.table[[2]]))^2)/nrow(x.table))
Σ       <- sum((x.table[[1]]-mean(x.table[[1]]))*(x.table[[2]]-mean(x.table[[2]])))
N.σx.σy <- nrow(x.table)*sd_x*sd_y
return(Σ/N.σx.σy)
}
#                Ответ:
P <- P.Pearson(x3); P
R2 <- P^2; R2
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M); M <- t(M); M <- as.data.frame(M);
M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA)
names(M) <- c("Factor", "Error", "Total"); M <- t(M)}
x3.anova<-x.anova(x3); x3.anova
SS_residual <- x3.anova[5]
SS_total    <- x3.anova[6]
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M); M <- t(M); M <- as.data.frame(M);
M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA)
names(M) <- c("Factor", "Error", "Total"); M <- t(M)}
x3.anova<-x.anova(x3); x3.anova
SS_residual <- x3.anova[5]
SS_total    <- x3.anova[6]
R2 <- 1 - (SS_residual/SS_total);  R2
SS_total
SS_residual
x.anova  <- function(x){
M <- lm(x[[1]] ~ x[[2]], data = x);
M <- anova(M); M <- t(M); M <- as.data.frame(M);
M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA)
names(M) <- c("Factor", "Error", "Total"); M <- t(M)}
x3.anova<-x.anova(x3); x3.anova
SS_residual <- x3.anova[5]
SS_total    <- x3.anova[6]
R2 <- 1 - (SS_residual/SS_total);  R2
P <- P.Pearson(x3); P
R2 <- P^2; R2
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.regr(x3)
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.disp(x3)
Anova.regr(x3)
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.regr(x3)
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.regr(x3)
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.regr(x3)
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.regr(x3)
Anova.regr(x3)
Anova.disp(x3)
Anova.disp(x1)
Anova.disp(x3)
Anova.regr(x3)
source("C:/Users/ak179/OneDrive/Магистратура/2 КУРС/1 СЕМЕСТР/Лекции (2 курс)/Язык R (шпоры и задачи)/Скрипты/Функции для R/R_test_func2.R")
Anova.regr(x3)
x.func.reg <- function(x) {
M <- lm(x[[1]] ~ x[[2]], data = x);
}
x.test <- x.func.reg(x3)
View(x.test)
aov(x.test)
anova(x.test)
x.func.1 <- function(x) {
M <- lm(x[[1]] ~ x[[2]], data = x);
}
x.test <- x.func.1(x3); anova(x.test)
x.func.2 <- function(x) {
M <- lm(x[[2]] ~ x[[1]], data = x);
}
x.test <- x.func.2(x3);anova(x.test)
Anova.regr(x3)
x.func.1 <- function(x) {
M <- lm(x[[1]] ~ x[[2]], data = x);
}
x.test <- x.func.1(x3); anova(x.test)
x.func.2 <- function(x) {
M <- lm(x[[2]] ~ x[[1]], data = x);
}
x.test <- x.func.2(x3);anova(x.test)
as.data.frame(anova(x.test))
x3[[2]]
x3$[[2]]
x3$[2]
x3$1
x3$2
x3$OGE
x.func.2 <- function(x) {
M <- lm([[2]] ~ [[1]], data = x);
x.func.2 <- function(x) {
M <- lm([[2]] ~ [[1]], data = x);
x.func.2 <- function(x) {
M <- lm(x[[2]] ~ x[[1]], data = x);
}
x.test <- x.func.2(x3);anova(x.test)
as.data.frame(anova(x.test))
