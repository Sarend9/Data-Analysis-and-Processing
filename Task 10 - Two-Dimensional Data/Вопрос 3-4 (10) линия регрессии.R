# Вопрос 3: Имеются результаты ОГЭ и ЕГЭ группы респондентов: exams.dat.
#
#           Постройте для этой группы модель регрессии балла ЕГЭ на
#           балл ОГЭ, исходя из линейности связи двух этих показателей.

file <- "3_4.exams.dat"
x3 <- read.csv(paste0(dirname(rstudioapi::getSourceEditorContext()$path), "/", file)); rm(file)

# -/МОРАЛЬ/-
# 1) "Проверь какие данные в переменной"
# 2) "Смотри сначала на диаграмму, прежде чем считать коэффициенты"  


# 1) Проверим данные:

#    1.1) Посмотрим в консоли что из себя предсатвяют данные:  
          print("начальные строки объекта:");     head(x3) 
          print("внутренняя структура объекта:"); str(x3)
     
#    1.2) Посмотрим график:         
          plot(x3)
          
#        -/ВЫВОД/-
#        В отличии от задания с дисперсионным анализом, здесь
#        обе переменные метрические, но мера связи по прежнему
#        будет служить коэффициент  η2 — с точки зрения вычислений 
#        принципиально ничего не изменится, только метод будет называться 
#        уже не дисперсионным, а регрессионным анализом.          
          
#    Задачи:
#    • Введите значение β1;
#    • Введите значение β0;
#    • Определите, насколько хорошо результат
#      одного экзамена предсказывает другой, через R2.  
          
   
# 2) Регрессионная модель
          
#    -/ОБЩАЯ ТЕОРИЯ/-          
#    В рамках регрессионного анализа существует возможность 
#    ПРЕДСКАЗЫВАТЬ значения одной из переменных по значениям другой, поэтому:
#    • переменную-“фактор”    - принято называть предиктором (predictor), 
#    • “зависимую” переменную - принято называть откликом (response variable).
          
#    • При ДВУМЕРНЫХ данных модель называют «простой регрессией», в ней:
#      o  один предиктор - “фактор”;         ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#      o  один отклик    - “зависимая” переменная.
          
#    • При МНОГОМЕРНЫХ данных модель называют «множественной регрессией», в ней:
#      o  несколько переменых предикторов;     ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
          
          
          
#    2.1) коэффициент детерминации R²
#         ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾   
#         Модели «дисперсионного» и «регрессионного» анализа основаны на одном и том же
#         принципе разложения изменчивости зависимой переменной (отклика) на две составляющие:            
#         • эффект, связанный с фактором (предиктором), 
#         • и нормально распределённые остатки. Обе эти модели — разновидности т.н. общей линейной модели    
#             
#         Для вычисления меры связи переменных в модели линейной регрессии мы 
#         можем воспользоваться тем, что в линейной модели общая дисперсия 
#         переменной-отклика равна сумме дисперсий эффекта и остатков:
          
#         σ²_total = σ²_effect + σ²_residual
          
#         Соотвественно, мы можем вычислить η2 двумя способами:
#         
#                 SS_effect        SS_residual
#         η2  =   —————————  = 1 - ———————————   
#                 SS__total         SS__total
#                                   
#         -/ВАЖНО/-
#         • Для «дисперсионного» способа - считать через SS_effect, т.к. есть факторы (есть группы)
#         • Для «регрессионного» способа - считать через SS_residual - сумма квадратов остатков
         
#         Т.к. в «регрессионном» анализе групп НЕТ, в нем мера связи называется
#         НЕ «коэффициент внутригрупповой корреляции η2», - а «коэффициент детерминации R²»
           
#                  SS_residual
#         R² = 1 - ———————————   
#                   SS__total  
          
#         -/Примечание/-          
#         В случае ЛИНЕЙНОЙ регрессии коэф. R² можно вычилсть еще R²=p², где p - коэф. линейн. кор. Пирсона.     
          
#         2.1.1) Через встроенную функцию найдем суммы квадратов:
                 x.anova  <- function(x){
                   M <- lm(x[[1]] ~ x[[2]], data = x);
                   M <- anova(M); M <- t(M); M <- as.data.frame(M);
                   M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA) 
                   names(M) <- c("Factor", "Error", "Total"); M <- t(M)}
                 
                 x3.anova<-x.anova(x3); x3.anova     
                 
                 SS_residual <- x3.anova[5]
                 SS_total    <- x3.anova[6]
                 
                 R2 <- 1 - (SS_residual/SS_total);  R2

                 P.Pearson <- function(x.table)
                 {
                   x.table <- na.omit(x.table)
                   sd_x    <- sqrt(sum((x.table[[1]] - mean(x.table[[1]]))^2)/nrow(x.table))
                   sd_y    <- sqrt(sum((x.table[[2]] - mean(x.table[[2]]))^2)/nrow(x.table))
                   Σ       <- sum((x.table[[1]]-mean(x.table[[1]]))*(x.table[[2]]-mean(x.table[[2]])))
                   N.σx.σy <- nrow(x.table)*sd_x*sd_y
                   return(Σ/N.σx.σy)
                 }
                 
#                Ответ:
                 P <- P.Pearson(x3); P 
                 R2 <- P^2; R2
                 
#    2.2) коэффициенты регрессии
#         ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾     
#         Нужно провести оптимальную линию регрессии через облако рассеяния. 
#         Самый простой способ: линия выбранной формы накладывается на данные 
#         (модель “подгоняется”) так, чтобы остатки (отклонения значений переменной-отклика)
#         от этой линии, оказались минимальными. 
#         Точнее, минимальными должны быть квадраты остатков (сумма этих квадратов) — 
#         такой способ подгонки модели называют «методом наименьших квадратов».
#                                                ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾               
#         По сути дела, построить, или подогнать регрессионную модель: 
#         найти её коэффициенты, β1 и β0 для простой линейной регрессии. 
#                
#                        | Коэффициент β1 определяет угол наклона линии регрессии.
#                σ_Y     | p - коэффициент линейной корреляции Пирсона.      
#         β1 = p ———     |              
#                σ_X     |   

#                              | Коэффициент β0 — свободный член, константа, или интерцепт 
#                              | представляет собой значение Yi при Xi = 0, т.е точку "оси y" 
#         β0 = μ_Y - (β1*μ_X)  | в месте её пересечения (intercept) линией регрессии, фактически           
#                              | величину сдвига всей линии регрессии по оси y относительно параллельной                    
#                              | прямой, проходящей через начало координат. 

                 
                 
          σ_X <- sqrt(sum((x3[[1]] - mean(x3[[1]]))^2)/length(x3[[1]])) 
          σ_Y <- sqrt(sum((x3[[2]] - mean(x3[[2]]))^2)/length(x3[[2]])) 
          
          β1  <- P*(σ_Y/σ_X) 
          β1
          
          μ_X <- mean(x3[[1]])
          μ_Y <- mean(x3[[2]])
          
          β0 = μ_Y - (β1*μ_X)
          β0
          
  
# СИНИЯ ЛИНИЯ                  
plot(x3, xlab ="ОГЭ, [баллы]", ylab = "ЕГЭ, [баллы]", main = "Диаграмма рассеяния экзаменов")

abline(β0, β1, col = "steel blue")
   

# ФИОЛЕТОВАЯ ЛИНИЯ       
β1.yx  <- β1 
β1.inverse <- 1/β1.yx  # замена коэффициента регрессии на обратную величину
β0.inverse <- μ_Y - (β1.inverse*μ_X)  # вычисление свободного члена
abline(β0.inverse, β1.inverse,
       col = 'maroon')

# ЗЕЛЕНАЯ ЛИНИЯ  
β1.full <- 1*(σ_Y/σ_X) 
β0.full <- μ_Y - (β1.full*μ_X)
abline(β0.full, β1.full,
                 col = 'green3',
                 lty = 'dotted', # пунктир
       lwd = 2)  # пунктир
          