# -/ОБЩАЯ ТЕОРИЯ/-
# ----------------
# Установить, какая из переменных влияет на другую, т.е. выявить, какая из них — причина, 
# а какая — следствие, статистическими методами невозможно в принципе. 
#
# Статистический анализ позволяет выявить лишь само наличие связи, 
# описать её выраженность и некоторые другие формальные характеристики.


# На вопрос о том, как установить, что обнаруженная связь — это именно влияние, т.е. между
# переменными существуют причинно-следственные отношения, отвечает «планирование эксперимента»


OR.Q <- function(x, transposition = FALSE) {
  
# -/ИНСТРУКЦИЯ/-  
# x - либо может быть обычной таблицей с 2 колонками. либо сразу таблицей сопряженности. 
# transposition - нужно ли траспонировать таблицу сопряженности (на результат не влияет!)
  
# 1) Для нормальной работы с данными следует избавиться от пропущенных строк (избавимся от NA):  
     if (((is.data.frame(x)) | (is.numeric(x))) & !is.table(x)) {
     x <- na.omit(x)
  
#    -/ПРИМЕЧАНИЕ/-:   
#    Функция "na.omit(x)" выдаёт исходный объект x, целиком опуская (omit) 
#    строки с пропусками хотя бы по одной переменной.
  
# 2) Таблица сопряжённости:
  
#    2.1) Составим таблицу сопряжённости по нашим данным:
  x <- table(x)}
#                                               |   В данной таблице сопряжённости две ГРАДАЦИИ: 
#         -/ПРИМЕР/-:                           |     • переменной (pet) по строкам - "кошки" и "собаки"
#                              married          |     • переменной (married) по столбцам  - "одинокий" и "в браке"
#                                               |   По столбцам; соответствующие частоты обозначили буквами a, b, c, d. 
#                      | одинокий | в браке |   |  
#               ------------------------------  |   Т.е. такие переменные называются БИНАРНЫМИ.
#               кошки  |    a     |    b    |   |   В случае двух бинарных переменных в таблице сопряжённости
#         pet   ------------------------------  |   оказывается четыре ячейки, или поля. 
#               собаки |    c     |    d    |   |         
#                                               |   Её так и называют — четырёхпольной (fourfold),
#                                               |   или таблицей 2 × 2 (two by two table).
#         
#         Симметричное отображение таблицы с помощью функции t() относительно диагонали.
          if(transposition == TRUE) {
          x <- t(x); cat("Таблица сопряженности (т): \n"); cat("-------------------------------------------------------"); cat("\n"); print(x); cat("-------------------------------------------------------" ,"\n"); cat("\n\n")
          } else {   cat("Таблица сопряженности:\n");      cat("-------------------------------------------------------"); cat("\n"); print(x); cat("-------------------------------------------------------" ,"\n"); cat("\n\n")}


#  3) Найдем:
#    • отношение шансов OR;
#    • коэффициент Q Юла (модуль).
#
#    3.1) Отношение шансов OR:
#         
#         Если обе переменные БИНАРНЫ, меру их связи легко получить, 
#         соотнеся шансы какого-нибудь из значений одного признака при 
#         разных значениях второго. 
#          
#         Пример поиска OR:
#                                               |                    | Так, мы можем сравнить шансы:
#                              married          |                    |  • a/c - увидеть кого-то c питомцем кошкой, наблюдая только за одинокими, 
##                                              | Формула шансов OR: |  • b/d — только за тех, кто в браке.
#                      | одинокий | в браке |   |                    |
#               ------------------------------  |      a/c   a*d     |
#               кошки  |    a     |    b    |   | OR = ——— = ———     | Сравнить шансы можно, вычислив их отношение — мы получим меру связи, 
#         pet   ------------------------------  |      b/d   b*c     | которая так и называется: отношение шансов (Odds Ratio, OR) 
#               собаки |    c     |    d    |   |                    | В последнем виде проясняется смысл отношения шансов (ad/bc). 
#                                               |                    |
#          
#        ad и bc — это диагонали таблицы 2 × 2, на которых представлены 
#        противоположные закономерности, так: 
#                                        • a — "одиночки" с питомцами кошками; 
#                                        • d — "в браке"  с питомцами собаками. 
#          
#        Если все наблюдения попадают на диагональ ad, то это полная, 
#        однозначная детерминистическая связь: у "одиночек" питомцы всегда являются кошками.
#          
#        Диагональ bc представляет обратную закономерность: питомцы являются кошками всегда у тех, кто "в браке".         
         
         a <- x[1,1];  b <- x[1,2]

         c <- x[2,1];  d <- x[2,2]
         
         cat ("Тенденции:", "\n")
         cat("-------------------------------------------------------" ,"\n")
         cat( "• диагональ ad - тенденция:", 
         "(", as.character((as.data.frame(x))[[1]])[1], "/", as.character((as.data.frame(x))[[2]])[1],"=", a, ")", 
         "и",
         "(", as.character((as.data.frame(x))[[1]])[4], "/", as.character((as.data.frame(x))[[2]])[4],"=", d, ")") 
         cat("\n")    
         cat( "• диагональ bc - тенденция:", 
         "(", as.character((as.data.frame(x))[[1]])[3], "/", as.character((as.data.frame(x))[[2]])[3],"=", b, ")", 
         "и",
         "(", as.character((as.data.frame(x))[[1]])[2], "/", as.character((as.data.frame(x))[[2]])[2],"=", c, ")") 
         cat("\n")    
         cat("-------------------------------------------------------" ,"\n")
         cat("\n\n")    
         
         
     # Отношение шансов OR:
         OR <- (a/c)/(b/d)
         

#    3.2) коэффициент Q Юла (модуль):        
#         
#                                 |  Отношение шансов хорошо свой простотой и понятностью, но имеет и недостатки.
#         Формула:                |  и недостатки. Когда связи нет, т.е. она равна нулю, OR = 1. Кроме         
##                                |  того, OR несимметрично: в меньшую сторону оно изменяется в  
#             OR - 1   ad - bc    |  пределе до нуля, а в большую — до бесконечности. Эти недостатки
#         Q = —————— = ———————    |  устраняются простым математическим фокусом: делим OR на само 
#             OR + 1   ad + bc    |  себя, при этом из числителя вычитаем единицу, а к знаменателю — 
##                                |  наоборот, прибавляем.
#         
#         ad - тенденция, что те, кто одиноки с питомцем кошкой и те, кто в браке с питомцем собакой
#         bc - тенденция, что те, кто в браке с питомцем кошкой и те, кто одиноки с питомцем собакой
#           
#         Если тенденции одинаковы, то получаем 0, если какая-то преобладает, то получаем 1 или -1.  
#         
#         Коэффициент Q:         
          Q <- (OR-1)/(OR+1)

          if (Q > 0) {trend <- "тенденция «ad» преобладает над тенденцией «bc»"}
          if (Q < 0) {trend <- "тенденция «bc» преобладает над тенденцией «ad»"}
          
# 4) Интепретация:

#    Таблица РАЗМЕРА ЭФФЕКТА:
#    ------------------------
#    модуль коэффициента	связь
#    <    0.10	не выражена
#    ≥    0.10	слабая
#    ≥    0.30	умеренная
#    ≥    0.50	тесная

#    В таблице приведены ориентировочные, достаточно условные уровни размера эффекта, предложенные Джейкобом Коэном (Cohen, 1988).

     if  (abs(Q) <  0.10)                    {power.com <- "не выражена"}
     if ((abs(Q) >= 0.10) & (abs(Q) < 0.30)) {power.com <- "слабая"     }
     if ((abs(Q) >= 0.30) & (abs(Q) < 0.50)) {power.com <- "умеренная"  }
     if  (abs(Q) >= 0.50)                    {power.com <- "тесная"     }


     cat('Сила связи между', '\033[1m', names(as.data.frame(x)[1]), '\033[0;0m',
         'и', '\033[1m', names(as.data.frame(x)[2]), '\033[0;0m', ":","\n")
     cat("-------------------------------------------------------"   ,"\n")
     cat("• OR - отношение шансов: ", round(OR, 6)                   ,"\n")
     cat("• коэффициент Q Юла    : ", round(Q, 6)                    ,"\n")
     cat("-------------------------------------------------------"   ,"\n")

     cat("• Вывод: 1) обнаружена ", '\033[1m', power.com ,'\033[0;0m', "связь", 
                      '\x1B[3m', names(as.data.frame(x)[1]), '\x1B[23m',
                      "с",
                      '\x1B[3m', names(as.data.frame(x)[2]), '\x1B[23m')
     cat("\n")      
     cat("         2)", trend)  
     cat("\n")   
     cat("-------------------------------------------------------" ,"\n")
}

Cramer.V <- function(x){
  
# 1) Найдем:
#    • [Теоретически] ожидаемые частоты;
#    • χ2 Пирсона;
#    • V  Крамера.
  
#    1.1) [Теоретически] ожидаемые частоты для модели НЕЗАВИСИМЫХ признаков:
  
#         1.1.1) Для нормальной работы с данными следует избавиться от пропущенных строк (избавимся от NA):
                 if (((is.data.frame(x)) | (is.numeric(x))) & !is.table(x)) {
                 x <- na.omit(x)
                 
#         1.1.1) Составим таблицу сопряжённости:
                 x <- table(x)}
      
                 cat("Таблица сопряжённости:"); cat("\n")
                 cat("-------------------------------------------------------"); cat("\n")
                 print(x)
                 cat("-------------------------------------------------------"); cat("\n"); cat("\n")
  

#         1.1.2) Переведём в относительные частоты:
                 relat.x.tbl <- x/sum(x)
  
#                -/ПРИМЕЧАНИЕ/-:    
#                Функция length() не подойдет, т.к. посчитает количество стобцов,
#                поэтому используем nrow() для получения количества строк.
#                Так же можно использовать sum() от таблицы сопряжённости.
  
  
#         1.1.3) Находим распределение вероятностей независимых признаков (вектора МАРГИНАЛЬНЫХ частот):
                 marg.rows <- vector()                                # создаем пустой вектор
                 marg.rows <- apply(relat.x.tbl, MARGIN = 1, FUN = sum)# суммы по строкам
                 marg.cols <- apply(relat.x.tbl, MARGIN = 2, FUN = sum)# суммы по столбцам
  
#                -/ПОЯСНЕНИЕ/-:
#                Что мы нашли? МАРГИНАЛЬНЫЕ ЧАСТОТЫ - marg.rows и marg.cols, слово МАРГИНО - записки на полях 
  
#                          |   ВКонтакте  | Одноклассники |   Фейсбук    |
#                -----------------------------------------------------------------------
#                mail.ru   |              |               |              |  marg.rows[1]
#                -----------------------------------------------------------------------
#                yandex.ru |              |               |              |  marg.rows[2]
#                -----------------------------------------------------------------------
#                gmail.com |              |               |              |  marg.rows[3]
#                -----------------------------------------------------------------------
#                          | marg.cols[1] |  marg.cols[4] | marg.cols[3] |      1
  
  
#         1.1.4) По маргинальным частотам вычисляем ОТНОСИТЕЛЬНЫЕ частоты для "РАСПРЕДЕЛЕНИЯ НЕЗАВИСИМЫХ ПРИЗНАКОВ":
#                (это попарные произведения маргинальных относительных частот одной и другой переменных).
                 model.prob <- marg.rows %o% marg.cols # (еще оно называется - внешнее произведение векторов)
                 
#                -/ПРИМЕЧАНИЕ/-:   
#                Операция " %o% " попарно перемножает каждый элемент одного вектора 
#                на все элементы другого (такое произведение называется внешним).
  
#                -/ПОЯСНЕНИЕ/-:                 
#                Пример того, как выглядит перемножение: 
  
#                          |          ВКонтакте          |        Одноклассники        |           Фейсбук           |
#                -------------------------------------------------------------------------------------------------------------------
#                mail.ru   | marg.rows[1] * marg.cols[1] | marg.rows[1] * marg.cols[2] | marg.rows[1] * marg.cols[3] |  marg.rows[1]
#                -------------------------------------------------------------------------------------------------------------------
#                yandex.ru | marg.rows[2] * marg.cols[1] | marg.rows[2] * marg.cols[2] | marg.rows[2] * marg.cols[3] |  marg.rows[2]
#                -------------------------------------------------------------------------------------------------------------------
#                gmail.com | marg.rows[3] * marg.cols[1] | marg.rows[3] * marg.cols[2] | marg.rows[3] * marg.cols[3] |  marg.rows[3]
#                -------------------------------------------------------------------------------------------------------------------
#                          |          marg.cols[1]       |          marg.cols[4]       |          marg.cols[3]       |      1
  
  
#         1.1.5) Переводим ожидаемые ОТНОСИТЕЛЬНЫЕ частоты в АБСОЛЮТНЫЕ, умножая model.prob 
#                на объём наблюдений, и сохраняем результат как model.freq
                 model.freq <- model.prob * sum(x)

                 cat("[Теоретические] ожидаемые частоты для модели независимости признаков:"); cat("\n")
                 cat("-------------------------------------------------------"); cat("\n")
                 print(model.freq)
                 cat("-------------------------------------------------------"); cat("\n"); cat("\n")
                 
  
  
#    1.2) χ2 Пирсона (квадраты стандартизованных остатков):
  
#         Сравним полученные "эмпирическое" и "теоретическое" распределения [абсолютных] частот,
#    
#         Формула χ2 Пирсона:   |            
#                               |  где:
#              k  (Oᵢ - Eᵢ)²    |  • Oᵢ - эмпирически наблюдаемая  (Observed) частота;
#         χ2 = Σ  ——————————    |  • Eᵢ - [теоретически] ожидаемая (Expected) частота;
#             i=1     Eᵢ        |  • разницу между "Observed" и "Expected" называют остатком (residual).
          
          
          Pirs.X2 <- sum(((x - model.freq)**2)/model.freq)
  
  
#    1.3) V Крамера:
  
#         У χ2 Пирсона так же, как у OR, есть недостатки: он может изменяться 
#         в пределах [0 : бесконечности], в зависимости от объёма наблюдений 
#         и от числа градаций признаков. 
  
#         Эти неудобства устраняют, нормируя χ2 на N и внося некоторые поправки — так 
#         получают новые меры связи, например, коэффициент V Крамера (Cramér’s V):
  
#         V Крамера принимает значения:
#          • ОТ "0" - при полном отсутствии связи между переменными 
#          • ДО "1" - при полной функциональной связи.
  
#         Формула V Крамера: 
#         
#                                         |  где:
#                    x²                   |  • l — меньшее из ГРАДАЦИЙ (количеств строк r и столбцов) c в таблице сопряжённости;
#         Vc = √(————————); l = min(r,c)  |  т.е. из (таблицы 3 X 2) l=2, (таблицы 5 X 3) l=3 и т.п.
#                 N(l-1)                  | 
  
          Cram.V <- sqrt(Pirs.X2/(sum(x)*(min(c(nrow(model.freq), (ncol(model.freq))))-1)))

# 2) Интепретация:

#    Таблица РАЗМЕРА ЭФФЕКТА:
#    ------------------------
#    модуль коэффициента	связь
#    <    0.10	не выражена
#    ≥    0.10	слабая
#    ≥    0.30	умеренная
#    ≥    0.50	тесная

#    В таблице приведены ориентировочные, достаточно условные уровни размера эффекта, предложенные Джейкобом Коэном (Cohen, 1988).

     if  (abs(Cram.V) <  0.10)                         {power.com <- "не выражена"}
     if ((abs(Cram.V) >= 0.10) & (abs(Cram.V) < 0.30)) {power.com <- "слабая"     }
     if ((abs(Cram.V) >= 0.30) & (abs(Cram.V) < 0.50)) {power.com <- "умеренная"  }
     if  (abs(Cram.V) >= 0.50)                         {power.com <- "тесная"     }


     cat('Сила связи между', '\033[1m', names(as.data.frame(x)[1]), '\033[0;0m',
         'и', '\033[1m', names(as.data.frame(x)[2]), '\033[0;0m', ":"          ,"\n")
     cat("-------------------------------------------------------"             ,"\n")
     cat("• коэффициент χ2 Пирсона:", round(Pirs.X2, 6)                        ,"\n")
     cat("• коэффициент V  Крамера:", round(Cram.V , 6)                        ,"\n")
     cat("-------------------------------------------------------"             ,"\n")

     cat("• Вывод: обнаружена ", '\033[1m', power.com ,'\033[0;0m', "связь", 
         '\x1B[3m', names(as.data.frame(x)[1]), '\x1B[23m',
         "с",
         '\x1B[3m', names(as.data.frame(x)[2]), '\x1B[23m');                cat("\n")
     cat("-------------------------------------------------------"             ,"\n")
}
  

Kendall.T <- function(x, full.print=FALSE){
  
  # full.print - печатает все итерации по подсчету Q
  
# 2) Найдем число инверсий Q;
  
#    2.1) Для удобства подсчёта сначала упорядочим наблюдения по возрастанию первой переменной, 
#         и далее будем анализировать только вторую
          i <- order(x[ , 1]); i# вектор номеров строк, упорядоченных по 1-му столбцу
  
#         -/ПРИМЕЧАНИЕ/-:
#         ---------------
#         Функция "order()" выдаёт номера всех строк таблицы experts 
#         в порядке возрастания значений столбца, указанного в фильтре
  
#         -/ПРИМЕР order()/-:
#         -------------------
#         Команда x[ , 1] выведет вектор tour.1, но у него есть порядок, определяемый индексами:  
#         индекс : 1 2 3 4 5 6 7 8
#         tour.1 : 7 5 3 4 6 1 2 8
  
#         Если значения x[ , 1] условно для себя представить в остортированном порядке, 
#         то индексы будут расположены следующим образом:
#         индекс : 6 7 3 4 2 5 1 8
#         tour.1 : 1 2 3 4 5 6 7 8
  
#         Команда order(x[ , 1]) как раз и выведет эти индексы:
#         i <- order(x[ , 1]) - это: 6 7 3 4 2 5 1 8
  
#    2.2) Отсортируем таблицу так, чтобы вектор номеров i задал новый порядок следования строк x
          x <- x[i, ]; x # строки таблицы в новом порядке
  
#    2.3) Напишем цикл для подсчета инверсий:    
  
#         При полном совпадении столбцов её значения должны 
#         также выстроиться в порядке возрастания, т.е. при попарном сравнении строк
#         значение ВЕРХНЕЙ строки всегда должно быть МЕНЬШИМ, чем НИЖНИЕЙ строки.
  
#         Если, наоборот, меньшим оказывается значение нижней строки, 
#         то мы получаем несогласованное изменение — инверсию.    
  
#         Код ниже написин таким образом, что в консоле отобразит полностью свою работу:
  
          if (full.print==TRUE){
                                                                cat("Таблица x: ", "\n\n"); t(x)
          Q <- numeric(1); P <- numeric(1)
         
          for (i in 1:(nrow(x) - 1)) {                           
                                                                cat('\033[1m', i, "цикл:", '\033[0;0m', "\n");                                         cat("--------------------------------------------","\n")
#         сохраняем столбец в отдельный вектор                                                         
          x.i  <- x[i:nrow(x), 2];                              cat("значения второй переменной:", x.i,  "\n");                                        cat("--------------------------------------------","\n")       
      
#         извлекаем только наблюдения, меньшие и больше первого                                                          
          inversionsQ   <- x.i[x.i < x.i[1]];                   cat("inversions   Q            :","меньше", x.i[1], "только:", inversionsQ   ,"\n");  cat("--------------------------------------------","\n")     
          coincidencesP <- x.i[x.i > x.i[1]];                   cat("coincidences P            :","больше", x.i[1], "только:", coincidencesP ,"\n");  cat("--------------------------------------------","\n")     
         
#         объём этих наблюдений, т.е. число инверсий                                                          
          Q1 <- Q + length(inversionsQ);                        cat("число инверсий   Q        :", Q1, "(",Q,"+",length(inversionsQ)  ,")","\n");     cat("--------------------------------------------","\n");
          P1 <- P + length(coincidencesP);                      cat("число совпадений P        :", P1, "(",P,"+",length(coincidencesP),")","\n");     cat("--------------------------------------------","\n"); cat("\n");
          Q <- Q1; P<-P1
          }
         
         }else{
         Q <- numeric(1)                                            
         for (i in 1:(nrow(x) - 1)) {                           
         x.i  <- x[i:nrow(x), 2]           # сохраняем столбец в отдельный вектор                
         inversions <- x.i[x.i < x.i[1]]   # извлекаем только наблюдения, меньшие первого                         
         Q <- Q + length(inversions)       # объём этих наблюдений, т.е. число инверсий     
         }
}
  
  
  
# 3) Найдем коэффициент Ꚍ-Кендалла:
# 
#    Коэффициент ранговой корреляции Кендалла Ꚍ (тау Кендалла, Kendall’s tau) 
#    — это разница между количеством согласованных и несогласованных изменений в парах наблюдений, 
#    нормированная на общее число таких пар:          
#         
#    Формула Ꚍ-Кендалла:
#
#        P - Q    |    где:      
#   𝛕 = —————    |    • P — согласованные изменения: при переходе от одного наблюдения к другому значения обеих переменных изменяются в одну и ту же сторону;
#        P + Q    |    • Q — несогласованные изменения, или инверсии. 
#
#          
#    Формула согласованных  | Формула (P) выведена из:
#    изменений P:           |
#                           |  
#        N*(N-1)            |           N*(N-1)
#    P = ——————— - Q        |   P + Q = ———————
#           2               |              2
  
  
     P <- (nrow(x)*(nrow(x)-1))/2 - Q
     T.Kendall <- (P-Q)/(P+Q)
  
#    Ꚍ-Кендалла также меняется в пределах  [–1 : 1] и равен 0 при отсутствии связи между переменными. 
#    В отличие от Q Юла, знак Ꚍ-Кендалла имеет четкую интерпретацию: он указывает НАПРАВЛЕНИЕ связи. 
#    Так, 
#         • (+) положительный знак указывает на прямое соотношение переменных:     при возрастании значений одной другая также растёт; 
#         • (-) отрицательный знак коэффициента указывает на обратное соотношение: росту одной переменной соответствует снижение значений другой.  
  
  
# 4) Интепретация:
  
#    Таблица РАЗМЕРА ЭФФЕКТА:
#    ------------------------
#    модуль коэффициента	связь
#    <    0.10	не выражена
#    ≥    0.10	слабая
#    ≥    0.30	умеренная
#    ≥    0.50	тесная
  
#    В таблице приведены ориентировочные, достаточно условные уровни размера эффекта, предложенные Джейкобом Коэном (Cohen, 1988).
  
     if  (abs(T.Kendall) <  0.10)                            {power.com <- "не выражена"}
     if ((abs(T.Kendall) >= 0.10) & (abs(T.Kendall) < 0.30)) {power.com <- "слабая"     }
     if ((abs(T.Kendall) >= 0.30) & (abs(T.Kendall) < 0.50)) {power.com <- "умеренная"  }
     if  (abs(T.Kendall) >= 0.50)                            {power.com <- "тесная"     }
  
  
     # наблюдения - то что по строкам, прим:   index tour.1 tour.2
     #                                           3     3      3
     # переменные - столбцы.
  
     cat('Сила связи между переменными:', '\033[1m', names(as.data.frame(x)[1]), '\033[0;0m',
         'и', '\033[1m', names(as.data.frame(x)[2]), '\033[0;0m', ":"                            ,"\n")
     cat("-------------------------------------------------------------"                         ,"\n")
     cat("• коэффициент Ꚍ Кендалла:", T.Kendall                    ,"\n")
     cat("• Q - число инверсий    :", Q                            ,"\n")
     cat("• P - число совпадений  :", P                            ,"\n")
     cat("-------------------------------------------------------------"                         ,"\n")
     
     if (T.Kendall>0){
     cat("• Вывод: 1) Ꚍ Кендалла", '\033[1m',  "положителен,",'\033[0;0m', "значит, связь между", "\n")  
     cat("            переменными (корреляция)", '\033[1m',  "прямая,",'\033[0;0m',"т.е. в ",     "\n") 
     cat("            наблюдениях чаще совпадения, чем расхождения\n")}
     
     if (T.Kendall<0){
     cat("• Вывод: 1) Ꚍ Кендалла", '\033[1m',  "отрицателен,",'\033[0;0m', "значит, связь между", "\n")   
     cat("            переменными (корреляция)", '\033[1m',  "обратная",'\033[0;0m',"т.е. в ",    "\n")
     cat("            наблюдениях чаще расхождения, чем совпадения\n")}
     cat("\n")
     cat("         2)",paste0("Ꚍ Кендалла = ", round(T.Kendall, 7), ", значит, что, беря наугад"),"\n")
     cat("            две пары наблюдений, мы обнаружим в наблюдениях: \n")
     cat("            • совпадение  в", paste0(round((0.5+(T.Kendall/2))*100,1), "%"), "случаев,   \n")
     cat("            • расхождение в", paste0(round((0.5-(T.Kendall/2))*100,1), "%"), "случаев.   \n")
     cat("\n")
     cat("         3) Обнаружена ", '\033[1m', power.com ,'\033[0;0m', "связь", 
         '\x1B[3m', names(as.data.frame(x)[1]), '\x1B[23m',
         "с",
         '\x1B[3m', names(as.data.frame(x)[2]), '\x1B[23m');          cat("\n")
     cat("-------------------------------------------------------------" ,"\n")

     # a.	Установить, какая из переменных влияет на другую статистическими методами невозможно в принципе. Статистический анализ позволяет выявить лишь само наличие связи, описать её выраженность и некоторые другие формальные характеристики. Т.е. QЮла>0.10 показывает лишь то, что есть слабая зависимость между переменными.
     # b.	А вот экспертный метод, например, ранговая корреляция позволяет определить влияние и направление корреляционной связи между двумя переменными.
     
}


Pearson.P <- function(x, transposition = FALSE){
  
  # transposition - меняет столбцы местами
  
  x[[1]]<-as.numeric(x[[1]]); x[[2]]<-as.numeric(x[[2]]);
  
  if (isTRUE(transposition)){a = 2; b = 1} else {a = 1; b = 2}
  
# 1) Построим диаграмму рассеивания:
     par(mfrow = c(1,1))
     plot(x,
          main = "Диаграмма рассеивания
                  исходных данных                  ",
          xlab = names(x[a]), 
          ylab = names(x[b]))
       
#    ВЫБРОСЫ хорошо видны на диаграмме в виде точек:
#    а) далеко отстоящих от общего облака рассеивания;
#    б) лежащих также далеко от линии идеальной линейной корреляции.
#    
#           Диаграмма рассеивания      
#           _____________________     
#       260|  *                 *|    
#          | * <-выбросы     * * |       
#    A  240|*            *   *   |       
#    n     |            *  *  *  |     
#    x  220|          *  * *     |    
#    i     |     *  *  *  *      |         
#    e  200|      *   *  *       |       
#    t     |   *   *             |     
#    y  180|    * *              |     
#          |  *                  |
#       160|                     |     
#           ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾      
#          70   85   100  115  130  
#
#                  IQ
#     
#    ρ Пирсона - коэффициент линейной корреляции ρ Пирсона
#   
#    Идеальная положит. кор-ция.      Идеальная отриц. кор-ция.        Нет линейной корреляции
#           ρ Пирсона = 1                   ρ Пирсона = -1                  ρ Пирсона = 0
#     _________________________       _________________________       _________________________            
#    |                        *|     |*                        |     |                         |             
#    |                      *  |     |  *                      |     |    *       *     *      |            
#    |                    *    |     |    *                    |     |       *             *   |              
#    |                  *      |     |      *                  |     |  *       *     *    *   |          
#    |                *        |     |        *                |     |      *                  |            
#    |              *          |     |          *              |     |  *        *    *     *  |            
#    |            *            |     |            *            |     |    *        *           |          
#    |          *              |     |              *          |     |          *       *      |        
#    |        *                |     |                *        |     |     *       *      *    |            
#    |      *                  |     |                  *      |     |  *    *        *        |             
#    |    *                    |     |                    *    |     |          *           *  |            
#    |  *                      |     |                      *  |     |  *     *   *      *     |          
#    |*                        |     |                        *|     |                         | 
#     ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾       ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾       ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾             



# 2) Удаление выбросов:

#    2.1) Определяем ИНДЕКСЫ точек интерактивно по графику:

#         Поскольку наши меры связи чувствительны к выбросам, 
#         мы решаем описать выбросы отдельно и удалить из 
#         дальнейшего анализа как не вписывающиеся в общую картину.

#         Для ОПИСАНИЯ и УДАЛЕНИЯ наблюдений нам нужно знать их индексы (номера строк).
         
          cat('\033[0;31;1m', "\n", "Выберете точки для удаления или нажмите ESC!",'\033[0;0m',"\n")
        
          identify <- identify(x)  # интерактивное определение индексов точек
         
          cat("\n", "Индексы выбросов:", '\033[0;31;1m', identify, '\033[0;0m', "\n")
          cat("\n", "Выбросы:", "\n"); print(x[identify, ]);cat("\n")
          
#         -/ПРИМЕЧАНИЕ/-:   
#         После применение команды "identify()" появится возможность      P.s: можно этот край просто закликать до предупреждения: 
#         "щелчком" отмечать прямо на диаграмме интересующие точки.       "ближайшая точка уже определена", а не точечно выцеливать
#         Для ОТМЕНЫ - нажать кнопку "Esc".
#         
#                 Диаграмма рассеивания      
#                 _____________________     
#             260|  *30               *|    
#                | *17  <- индексы, а не "x" или "y" ! 
#          A  240|*5                   |     
#          n     |            *  *  *  |     
#          x  220|          *  * *     |    
#          i     |     *  *  *  *      |         
#          e  200|      *   *  *       |       
#          t     |   *   *             |     
#          y  180|    * *              |     
#                |  *                  |
#             160|                     |     
#                 ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾      
#                70   85   100  115  130
#         
#                          IQ 
#         
#         Результат в Console:
#         [1]  5 17 30
          
    
#    2.2) Очистим таблицу данных от этих наблюдений (выбросов):
          if (length(identify)==0){x.clean <- x}else{
          x.clean <- x[-identify, ]}

#         -/ПРИМЕЧАНИЕ/-:             
#         Вектор со знаком “минус” составляем из индексов наблюдений-выбросов,
#         подлежащих исключению — их фильтр [] отбрасывает.
#         Индекс столбца, j не указан (после запятой пусто) — берутся оба столбца таблицы.
#         Результат сохраняем в новой таблице x.clean

#                Диаграмма рассеивания      
#                _____________________     
#            260|                    *|    
#               |   <- выбросы были очищены!    
#         A  240|                     |     
#         n     |            *  *  *  |     
#         x  220|          *  * *     |    
#         i     |     *  *  *  *      |         
#         e  200|      *   *  *       |       
#         t     |   *   *             |     
#         y  180|    * *              |     
#               |  *                  |
#            160|                     |     
#                ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾      
#               70   85   100  115  130   
#         
#                         IQ 

#    2.3) Убедимся, что выбросы исчезли:
          par(mfrow = c(1, 3))
               
          plot(x = x[[a]],
               y = x[[b]],
               main = "Диаграмма рассеивания
            исходных данных            ",
               xlab = names(x[a]), 
               ylab = names(x[b]),
     
               points(x[identify, 1], y = x[identify, 2], pch = 19, col='red', cex=1)
          )
          
          plot(x = x.clean[[a]],
               y = x.clean[[b]],
               main = "Диаграмма рассеивания, 
                       очищенная от выбросов                       ",
               xlab = names(x.clean[a]), 
               ylab = names(x.clean[b]))
    
          
#    2.4) Перед расчетами посмотрим сводку по таблице и проверим её:                 
#         summary(x.clean)

#         -/ПРИМЕР/-: 
#         IQ           Anxiety     
#         Min.   : 73.0   Min.   :156.0  
#         1st Qu.: 90.5   1st Qu.:191.0  
#         Median :101.0   Median :202.0  
#         Mean   :101.6   Mean   :202.8  
#         3rd Qu.:108.0   3rd Qu.:211.5  
#         Max.   :133.0   Max.   :253.0
#         -----------------------------
#         NA's   :2       NA's   :2   
#         -----------------------------

#    2.5) Для нормальной работы с данными избавимся от пропущенных строк (избавимся от NA):               
          x.clean <- na.omit(x.clean); # summary(x.clean)  

#         -/ВАЖНО/-:
#         НЕ ПРИМЕНЯТЬ функцию na.omit() ДО очистки от выбросов, иначе индексы сдвинуться!          


# 3) Определите, насколько в данных сильно связаны при помощи:
#    • ковариации Cov;          
#    • коэффициента ρ (коэффициент линейной корреляции ρ Пирсона);
#    • коэффициента ρs (коэффициент ранговой корреляции ρs Спирмена).
#
#
#    -/ОБЩАЯ ТЕОРИЯ/-
#    Корреляция, корреляционная зависимость — взаимозависимость двух или нескольких случайных величин. 
#    Суть ее заключается в том, что при изменении значения одной переменной происходит закономерное 
#    изменение (уменьшению или увеличению) другой(-их) переменной(-ых).   
#     
#    У корреляции есть три важных характеристики:
#    •	тип, или форма (линейная/варианты нелинейной);
#    •	выраженность (теснота, сила)
#    •	у линейной и монотонной корреляций — направление связи (прямая/обратная, или +/–).
#         
#
#    3.1) Ковариация Cov (мера совместной изменчивости (ЛИНЕЙНОЙ связи) двух случайных величин):
#
#         Мы будем выяснять, какая из тенденций преобладает: 
#         • переменные изменяются однонаправленно  - ковариация положительная (+) (при росте одной вторая тоже увеличивается); 
#         • переменные изменяются разнонаправленно - ковариация отрицательная (-) (если одна растёт, вторая уменьшается).
#
#         Для сопоставления тенденций разделим наблюдения каждой переменной на две группы: 
#         • выше среднего;
#         • ниже среднего.   
#         
#           Диаграмма рассеивания      Это удобно сделать, ЦЕНТРИРОВАВ переменные, где: 
#           _____________________      • в группу "выше среднего" — войдут положительные значения переменной; 
#          |"2"*      |      *"1"|     • в группу "ниже среднего" — войдут отрицательные значения переменной.           
#         6| *        |   *  *   |    
#          |*         |* *  *    |     Если распределение симметрично, то наблюдений в двух группах окажется примерно ПОРОВНУ.        
#         3|          |*  *      |     
#          |      * * |  * *     |     Комбинируя знаки двух переменных, всего мы получим четыре группы наблюдений,         
#         0|------*-- |-*--*-----|     которые на диаграмме оказываются в разных квадрантах (четвертях) системы координат:          
#          |     *  * |  *       |       
#        -3|   * * *  |          |     Наблюдения 1-го и 3-го квадрантов представляют тенденцию прямой связи переменных:      
#          |  * * *   |          |     • обе отклоняются от центра в одну сторону (обе в большую либо в меньшую).        
#        -6|  *       |          |
#          |"3"       |       "4"|     Наблюдения 2-го и 4-го представляют соответственно обратную тенденцию:   
#           ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾      • переменные отклоняются в противоположные стороны.
#          -20  -10   0   10   20
#         
#         Ковариация отражает как частоту, так и величину проявлений каждой тенденции.

#         Знак и величина ковариации указывают на:
#         ----------------------------------------
#         • преобладающую тенденцию;
#         • на степень её выраженности;
#         • силу.

#         Формула:                           |   -/Ковариация простыми словами/- 
#                    N                       |   -------------------------------    
#                    Σ  (Xᵢ - μₓ)(yᵢ - μᵧ)   |   Ковариация — это мера того, как две случайные величины изменятся при сравнении друг с другом.
#                   i=1                      |   ковариация — это разница между двумя тенденциями: прямым и обратным соотношением признаков.
#         Cov x,y = ——————————————————————   |   • Положительная ковариация - переменные одновременно увеличиваются или уменьшаются.
#                             N              |   • Отрицательная ковариация - одна переменная растёт, вторая уменьшается.
#                                            |   • Нулевая       ковариация - отсутствие линейной взаимосвязи.
#          
#         Ковариация может измерять движения двух переменных, но не указывает на степень, 
#         в которой эти две переменные изменяются по отношению друг к другу.
  
  
          Cov_x.y <- function(x.table)
          {
            x.table <- na.omit(x.table)
            Σ <- sum(                                # сложив все эти произведения, мы получим разность двух сумм:
              (x.table[[1]]-mean(x.table[[1]])) *    # (а) произведений отклонений в одном направлении;
                (x.table[[2]]-mean(x.table[[2]]))    # (б) произведений отклонений в противоположных.
            ) 
            N <- nrow(x.table)  # Нормируем эту разность на объём наблюдений N - найдём среднее произведение отклонений переменных по каждому наблюдению (ковариация)
            return(Σ/N)
          }
          Cov_x.y(x.clean)
          Cov_x.y(x)
  
#         -/ПРИМЕЧАНИЕ/-:  
#         Заметим, что формула КОВАРИАЦИИ похожа на формулу ДИСПЕРСИИ.
#         Если линейной связи между переменными нет, то ни одна из тенденций не преобладает (xᵢ=yᵢ), => (Cov = σ2).
#         Таким образом, ковариацию можно рассматривать как совместную дисперсию двух случайных величин, 
#         а дисперсию — как частный случай ковариации, когда рассматривается ковариация величины самой с собой.


#    3.2) Коэффициент ρ (коэффициент линейной корреляции ρ Пирсона):         

#         У "ковариации" есть ПРОБЛЕМА - она зависи от масштаба каждой из переменных.

#         -/ПРИМЕР/-: 
#         • если возьмем   x в [мм] - одно   значение ковариации;
#         • если переведем x в [м]  - другое значение ковариации.

#         Кроме того, в таком виде нельзя сравнивать силу связи пар переменных, 
#         имеющих разные единицы измерения.

#         Это не очень удобно, поэтому в дополнение к центрации ещё проводим НОРМИРОВАНИЕ, 
#         т.е. стандартизуем обе переменные. Т.е. просто делим еще и на стандартыне отклонения x и y.
#
#                  N  (Xᵢ - μₓ) (yᵢ - μᵧ)    N                       |   -/Кратко о "ρ Пирсона"/- 
#                  Σ  ————————— —————————    Σ  (Xᵢ - μₓ)(yᵢ - μᵧ)   |   -------------------------------
#                 i=1    σx        σy       i=1                      |   максимальное значение ρ Пирсона = "1"  (полная положительная линейная корреляция);
#         P x,y = ——————————————————————— = ——————————————————————   |   минимальное  значение ρ Пирсона = "−1" (полная отрицательная линейная корреляция);
#                            N                     N σx σy           |   среднее      значение ρ Пирсона = "0"  (отсутсвие линейной корреляции).

          P.Pearson <- function(x.table)
          {
            x.table <- na.omit(x.table)
            sd_x    <- sqrt(sum((x.table[[1]] - mean(x.table[[1]]))^2)/nrow(x.table))
            sd_y    <- sqrt(sum((x.table[[2]] - mean(x.table[[2]]))^2)/nrow(x.table))
            Σ       <- sum((x.table[[1]]-mean(x.table[[1]]))*(x.table[[2]]-mean(x.table[[2]])))
            N.σx.σy <- nrow(x.table)*sd_x*sd_y
            return(Σ/N.σx.σy)
          }

          P.Pearson(x.clean)
          P.Pearson(x)


#         Резюме по ковариации и линейной корреляции Пирсона:
#         ---------------------------------------------------
#         Ковариация и коэффициент линейной корреляции Пирсона описывают выраженность, или тесноту линейной связи,
#         т.е. то, насколько одна переменная имеет тенденцию изменяться прямо или обратно пропорционально другой.

#         В частности, на диаграмме ρ Пирсона описывает, насколько точки данных 
#         близко расположены к воображаемой прямой, которую можно через них провести. 

#         Если оптимальная линия, вокруг которой расположены точки, не прямая, 
#         ковариация и коэффициент линейной корреляции перестают быть адекватными мерами связи.



#    3.2) Коэффициент ρs (коэффициент ранговой корреляции ρs Спирмена).

#         3.2.1) По диаграмме видно, что оптимальная линия, вокруг которой расположены точки, не прямая. 
#                Значит ковариация и коэффициент линейной корреляции перестают быть адекватными мерами связи.

#                Такая нелинейная связь называется - МОНОТОННОЙ.
#            
#                Нелинейная монотонная связь         
#                 _______________________      
#                |*                      |     Функция - МОНОТОННАЯ, если направление изменения одной переменной       
#                |*                      |     не меняется с возрастанием (убыванием) другой переменной.   
#                |*                      |     
#                |*                      |     Монотонная связь может быть:
#                |  *                    |     ----------------------------        
#                |  *                    |     • либо неубывающей;
#                |    *                  |     • либо невозрастающей.               
#                |    *                  |         
#                |      *                |     Несмотря такую полную связь, на отсутствие разброса точек вокруг воображаемой линии,
#                |        * *            |     коэффициент их линейной корреляции вновь оказывается по модулю значительно меньше единицы.         
#                |            * *        |             
#                |                * * * *|     При монотонной связи часто распределения переменных имеют выраженную асимметрию, причём её знаки различаются.      
#                 ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾    

#                Коэффцицент кореляции "Пирсона p" может выйти здесь сильно заниженным, хотя точки почти идут ровно по линии.
#                Это говорит о том, что здесь данная мера не подходит для данного распределения.

#                
#         3.2.2) Для того, чтобы правильно применить коэффициент кореляции, нужно ВЫПРЯМИТЬ наши данные.
#                Т.е. нужно искуственно выпрямляем распределение, чтобы она налезала на одну прямую и считаем коэффициент корреляции.                  
#
#                Простой способ избавиться от асимметрии — перевести значения наблюдений каждой переменной в ранги: 
#                так мы получим практически равномерные распределения вида “1, 2, 3, 4 …,” 
#                где каждое значение будет встречаться лишь однажды, за исключением связанных рангов.
#
          
                 x.clean.rang <- data.frame(a = rank(x.clean[[a]]),
                                            b = rank(x.clean[[b]]))
                 
#                Сразу делаем ранги для данных с выбросами                           
                 x.rang       <- data.frame(a = rank(x[[a]]),
                                            b = rank(x[[b]]))
                 
                 names(x.clean.rang) <- c(names(x.clean[a]), names(x.clean[b]))
                 names(x.rang)       <- c(names(x.clean[a]), names(x.clean[b]))
                 
#         3.2.3) Затем вычислим коэффициент корреляции по той же формуле Пирсона для этих ранжированных данных 
#                — он будет называться уже коэффициентом ранговой корреляции Спирмена ρs (Spearman’s rank correlation coefficient). 

                 P.Pearson(x.clean.rang)  # корреляции Спирмена ρs для очищенных данных
                 
                 P.Pearson(x.rang)        # корреляции Спирмена ρs для данных с выбросами


#                -/ПРИМЕР РЕЗУЛЬТАТА/-:
#                                                                                            
#                            Монотонная связь                    Ранжированные данные           
#                   700  _______________________           35  ________________________    
#                       |*                      |             | *                      |   Коэффициент корреляции Спирмена применяется при работе     
#                       |**                     |       в  30 |   *  *                 |   с косвенными данными, которые формально являются порядковыми, 
#                в  500 |*                      |       р     |  * *  * *              |   но при этом мы считаем, что после перевода их шкалы в ранговую 
#                р      |*                      |       е  25 |     *  *               |   расстояния между такими рангами имеют приблизительный смысл интервалов. 
#                е      |**                     |       м     |    *   *  *            |   
#                м  300 | **                    | --->  я  20 |       *  *             |   В этом случае ρs обычно предпочтительнее τ Кендалла, 
#                я      |  *                    |       ,     |           *  *         |   поскольку учитывает не только направление совместного изменения переменных, 
#                ,      |   *                   |       р  15 |               *        |   но и плечо отклонения от центра, измеренное в рангах. 
#                м  100 |     *                 |       а     |               * *      |   τ Кендалла применяется соответственно при строго порядковых данных.
#                и      |        *              |       н  10 |                   *    |   
#                н      |           *           |       г     |                     *  |   Интерпретировать ρs можем как коэффициент линейной корреляции, который получили бы,        
#                     0 |                      *|       и   5 |                       *|   если бы выпрямили кривую монотонной связи вместе с рассеянными вокруг неё точками данных.           
#                        ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾               ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾  
#                       30  35  40  45  50  55  60          0   5   10  15  20  25  30 35
#                                 пробы, %                            пробы, ранги
#                
#                -/ПРИМЕЧАНИЕ/-:
#                Преимущество ρs ощутимо только при достаточно тесной связи переменных, когда её криволинейность хорошо выражена. 
#                По мере увеличения разброса точек воображаемая оптимальная линия, вокруг которой они расположены, становится всё менее определённой, 
#                и разница между линейной и монотонной формами корреляции перестаёт быть заметной. 
#                При ρ < 0.5 величина этого коэффициента нередко оказывается даже несколько больше, чем ρs.       




# 1) Построим диаграмму рассеяния ранжированных переменных 
     plot(x = x.clean.rang[[a]],
          y = x.clean.rang[[b]],
          main = "Ранжированные результаты",
          xlab = names(x.clean.rang[a]), 
          ylab = names(x.clean.rang[b]))
     
     par(mfrow = c(1,1))


# 4) Интепретация:

#    Таблица РАЗМЕРА ЭФФЕКТА:
#    ------------------------
#    модуль коэффициента	связь
#    <    0.10	не выражена
#    ≥    0.10	слабая
#    ≥    0.30	умеренная
#    ≥    0.50	тесная

#    В таблице приведены ориентировочные, достаточно условные уровни размера эффекта, предложенные Джейкобом Коэном (Cohen, 1988).

     if  (abs(P.Pearson(x.clean)) <  0.10)                                           {power1.com <- "не выражена"}
     if ((abs(P.Pearson(x.clean)) >= 0.10) & (abs(P.Pearson(x.clean)) < 0.30))           {power1.com <- "слабая"     }
     if ((abs(P.Pearson(x.clean)) >= 0.30) & (abs(P.Pearson(x.clean)) < 0.50))           {power1.com <- "умеренная"  }
     if  (abs(P.Pearson(x.clean)) >= 0.50)                                           {power1.com <- "тесная"     }
     
     if  (abs(P.Pearson(x.clean.rang)) <  0.10)                                      {power2.com <- "не выражена"}
     if ((abs(P.Pearson(x.clean.rang)) >= 0.10) & (abs(P.Pearson(x.clean.rang)) < 0.30)) {power2.com <- "слабая"     }
     if ((abs(P.Pearson(x.clean.rang)) >= 0.30) & (abs(P.Pearson(x.clean.rang)) < 0.50)) {power2.com <- "умеренная"  }
     if  (abs(P.Pearson(x.clean.rang)) >= 0.50)                                      {power2.com <- "тесная"     }


# наблюдения - то что по строкам, прим:   index tour.1 tour.2
#                                           3     3      3
# переменные - столбцы.

cat('Сила связи между переменными:', '\033[1m', names(as.data.frame(x)[a]), '\033[0;0m',
    'и', '\033[1m', names(as.data.frame(x)[b]), '\033[0;0m', ":"                            ,"\n")

cat("-------------------------------------------------------------"                         ,"\n")
cat("a) \033[34mКоэффициенты по очищенным данным:\033[0m"                                                  ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                                  ,"\n")
cat("   • мера линейной связи двух величин ковариация Cov:", Cov_x.y(x.clean)               ,"\n")
cat("   • коэффициент линейной корреляции ρ  Пирсона     :", P.Pearson  (x.clean)               ,"\n")
cat("   • коэффициент ранговой корреляции ρs Спирмена    :", P.Pearson  (x.clean.rang)          ,"\n")
cat("-------------------------------------------------------------"                         ,"\n")
cat("б) \033[34mКоэффициенты по данным с выбросами:\033[0m"                                                ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                                ,"\n")
cat("   • мера линейной связи двух величин ковариация Cov:", Cov_x.y(x)                     ,"\n")
cat("   • коэффициент линейной корреляции ρ  Пирсона     :", P.Pearson  (x)                     ,"\n")
cat("   • коэффициент ранговой корреляции ρs Спирмена    :", P.Pearson  (x.rang)                ,"\n")
cat("-------------------------------------------------------------"                         ,"\n")

  cat("• Вывод: 1) \033[34mЕсли модель ЛИНЕЙНА:\033[0m"                                                    ,"\n") 
  cat("            ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                         ,"\n")
if (P.Pearson(x.clean)>0){
  cat("             1.1) Коэффициент ρ  Пирсона", '\033[1m',  "положителен,",'\033[0;0m', "т.е.",  "\n")  
  cat("                  переменные изменяются", '\033[1m',  "однонаправленно.",'\033[0;0m', "\n")}

if (P.Pearson(x.clean)<0){
  cat("             1.1) Коэффициент ρ  Пирсона", '\033[1m',  "отрицателен,",'\033[0;0m', "т.е.",  "\n")  
  cat("                  переменные изменяются", '\033[1m',  "разнонаправленно.",'\033[0;0m', "\n")}
  cat("\n")
  cat("             1.2) Обнаружена ", '\033[1m', power1.com ,'\033[0;0m', "связь между", 
      '\x1B[3m', names(as.data.frame(x)[1]), '\x1B[23m',
      "и",
      '\x1B[3m', names(as.data.frame(x)[2]), '\x1B[23m');          cat("\n")
  cat("-------------------------------------------------------------" ,"\n")
  
  
  
  
    cat("•        2) \033[34mЕсли модель НЕЛИНЕЙНАЯ МОНОТОННАЯ:\033[0m"                         ,"\n")
    cat("            ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                         ,"\n")
if (P.Pearson(x.clean.rang)>0){
    cat("             1.2) Коэффициент ρs  Спирмена", '\033[1m',  "положителен,",'\033[0;0m', "т.е.",  "\n")  
    cat("                  переменные изменяются", '\033[1m',  "однонаправленно.",'\033[0;0m', "\n")}
  
if (P.Pearson(x.clean.rang)<0){
    cat("             1.2) Коэффициент ρs  Спирмена", '\033[1m',  "отрицателен,",'\033[0;0m', "т.е.",  "\n")  
    cat("                  переменные изменяются", '\033[1m',  "разнонаправленно.",'\033[0;0m', "\n")}
    cat("\n")
    cat("             2.2) Обнаружена ", '\033[1m', power2.com ,'\033[0;0m', "связь между", 
        '\x1B[3m', names(as.data.frame(x)[1]), '\x1B[23m',
        "и",
        '\x1B[3m', names(as.data.frame(x)[2]), '\x1B[23m');          cat("\n")
    cat("-------------------------------------------------------------" ,"\n")
    
  cat("\n")

  


# a.	Установить, какая из переменных влияет на другую статистическими методами невозможно в принципе. Статистический анализ позволяет выявить лишь само наличие связи, описать её выраженность и некоторые другие формальные характеристики. Т.е. QЮла>0.10 показывает лишь то, что есть слабая зависимость между переменными.
# b.	А вот экспертный метод, например, ранговая корреляция позволяет определить влияние и направление корреляционной связи между двумя переменными.









}



Anova.disp <- function(x, main.plot="Диаграмма", xlab.plot = names(x)[1], ylab.plot = names(x)[2]){
  
  x <- na.omit(x)
  
  if ((is.character(x[[1]])) | (is.character(x[[2]]))) {
  if (!is.character(x[[1]])){
    x <- x[c(2,1)]
    }}
  
  else if ((is.factor(x[[1]])) | (is.factor(x[[2]]))) {
  if (!is.factor(x[[1]])){
    x <- x[c(2,1)]
  } 
  }
  
  
# 1) Задача: Определите, насколько данных сильно связаны через:
#    • коэффициент Ppb (коэффициент точечно-бисериальной корреляции Ppb);
#    • коэффициент η2  (коэффициент внутригрупповой корреляции η2).
#
#    -/ОБЩАЯ ТЕОРИЯ/-
#    Согласованные изменения могут связывать не только пару переменных, чьи шкалы
#    имеют ОДИНАКОВУЮ мощность, но и комбинации двух переменных, чья мощность шкал не совпадает
#   
#    Может быть дополнительная задача ПРЕДСКАЗЫВАТЬ значения одной из переменных 
#    по значениям второй. Для этого в стат. анализе применяются модели, которые 
#    дополняя данные теоретической информацией, позволяют получать более детальные результаты. 
#       
#    Есть два таких базовых инструмента: 
#    • модель дисперсионного анализа; 
#    • регрессионную модель.
#            
#    Модели «дисперсионного» и «регрессионного» анализа основаны на одном и том же
#    принципе разложения изменчивости зависимой переменной (отклика) на две составляющие:            
#    • эффект, связанный с фактором (предиктором), 
#    • и нормально распределённые остатки. Обе эти модели — разновидности т.н. общей линейной модели    
#
#  
#    1) Коэффициент точечно-бисериальной корреляции Ppb  
#       ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#       Коэффициента Ppb представляет собой частный случай коэффициента p Пирсона 
#       и используется, когда одна из переменных измеряется в интервальной шкале 
#       или в шкале отношений, а другая переменная - в бинарной шкале.
#        
#       К сожалению, он не так удобен в интерпретации, как ρ, поскольку его максимальная 
#       абсолютная величина оказывается меньше 0.9 и дополнительно снижается  по мере 
#       того, как распределение бинарной переменной удаляется от равномерного.

#       Переведем номинальные значения в факторы, т.к.
#       не сможем нормально работать с символьным типом:
        x[[1]] <- factor(x[[1]])

#       1.1) Для работы с переменными, переведем факторный тип в числовой:
#            Т.к. по заданию нужно определить различие различие 
        
#                 между вариантами A и B, нужно перекодировать две градации   
#                 в нули и единицы, а третью (если есть) градацию исключить из анализа. 
#     
#                 Для этого создадим перекодированную копию таблицы данных,
#                 т.к. переменная с трем уровнями может еще пригодиться:         
                  x_Ppb <- x
                  # length(levels(x[[1]]))
                  # if (length(levels(x[[1]]))>2){
                  # a <- c(0:(length(levels(x[[1]]))-1))
                  # for (i in 3:length(levels(x[[1]]))){a[i]<-NA}
                  # levels(x_Ppb[[1]]) <- a
                  # }else {levels(x_Ppb[[1]]) <- c(0,1)}
             
#       1.2) Мы перекодировали в цифры только первые две (по алфавиту) градации, 
#            а третью градацию перевели в NA - в пропущенные значения. 
#            Однако при этом переменная остаётся в номинальном формате фактора. 
#            Для её перевода в числовой формат используем функцию as.numeric():

             x_Ppb[[1]] <- as.numeric(x_Ppb[[1]])  # перевод фактора в числовой формат

#       1.3) Т.к. мы исключили третью градацию, нужно удалить все связанные с ней данные:
             colSums(is.na(x_Ppb)) # Есть ли пропущенные значения
             x_Ppb <- na.omit(x_Ppb)
             
#       1.4) Т.к. выше писалось, что коэффициента Ppb - частный случай коэффициента p Пирсона,
#            только с когда одна переменная в бинарной, другая в интервальной/отношений, то
#            используем обычную функцию для поиска коэффициента Пирсона:                 

#                     N  (Xᵢ - μₓ) (yᵢ - μᵧ)    N                       |   -/Кратко о "ρ Пирсона"/- 
#                     Σ  ————————— —————————    Σ  (Xᵢ - μₓ)(yᵢ - μᵧ)   |   -------------------------------
#                    i=1    σx        σy       i=1                      |   максимальное значение ρ Пирсона = "1"  (полная положительная линейная корреляция);
#            P x,y = ——————————————————————— = ——————————————————————   |   минимальное  значение ρ Пирсона = "−1" (полная отрицательная линейная корреляция);
#                               N                     N σx σy           |   среднее      значение ρ Пирсона = "0"  (отсутсвие линейной корреляции).
             P.Pearson <- function(x.table)
             {
               x.table <- na.omit(x.table)
               sd_x    <- sqrt(sum((x.table[[1]] - mean(x.table[[1]]))^2)/nrow(x.table))
               sd_y    <- sqrt(sum((x.table[[2]] - mean(x.table[[2]]))^2)/nrow(x.table))
               Σ       <- sum((x.table[[1]]-mean(x.table[[1]]))*(x.table[[2]]-mean(x.table[[2]])))
               N.σx.σy <- nrow(x.table)*sd_x*sd_y
               return(Σ/N.σx.σy)
             }

#            Ответ:
             Ppb <- P.Pearson(x_Ppb)

#    2) Коэффициент внутригрупповой корреляции η2  (дисперсионный анализ) (ANOVA)
#       ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾          
#       Для описания того, насколько различается значения не с двух градаций, а СО ВСЕХ 
#       трёх градаций, используем коэффициент внутригрупповой корреляции η2.
#    
#       Суть дисперсионного анализа в том, что хотим "сравнить разброс средних", т.е. на графике 2, 
#       нас могут интересовать отдельные разбросы групповых средних (отмеченные стрелками ↑↓) 
#       относительно общего группового среднего разброса (линия по центру).
#       
#                 Диаграмма размахов 1                 Диаграмма размахов 2     
#               ________________________             ________________________                   
#         10.0 |                        |      10.0 |                        |                         
#              |                        |           |                        |                        
#              |           *            |           |           *↑           |                         
#          7.5 |                        |       7.5 |            ↓           |                         
#              |           *      *     |           |          ↑*      *↑    |                         
#              |                        |           |          ↓        ↓    |                        
#       Y    5 |           *      *     |     Y   5 |——————————-*——————*—————|                         
#              |     *                  |           |     *↑            ↑    |                         
#              |                  *     |           |      ↓           *↓    |                         
#          2.5 |     *                  |       2.5 |     *                  |                              
#              |                        |           |                        |                                 
#              |     *                  |           |     *                  |                       
#            0 |                        |         0 |                        |                                 
#               ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾             ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾                          
#                    A     B      C                        A     B      C                                  
#                                                                                                          
#                          X                                     X   
#             
#       Мера связи (этта): в данной формуле дисперсия эффекта делиться на общую дисперсию,
#       ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾  т.е. сравниваем НЕ разброс средних с разбросом внутри каждой группы,
#                          а просто сравниваем разброс средних с ОБЩИМ разбросом (общей дисперсией).                
#             k                                                   
#             Σ  n_j(μ_j - μ_total)²                     | где:         
#            j=1                          SS_effect      | • k   - число групп; 
#       η² = ———————————————————————  =  ———————————     | • n_j - число наблюдений в j-той группе;
#             N                           SS_total       | • μ_j - групповое среднее;
#             Σ    (X_i - μ_total)²                      | • μ_total - общее среднее;
#            i=1                                         | • X_i - все наблюдения по всем группам;

#       2.1) Для нахождения коэффициента корреляции η2 
#            начнём с вычисления «групповых средних».

#            Перекодируем все 3 градации (ОПЦИОНАЛЬНО):
             x_η2 <- x
             # levels(x_η2[[1]]) <- c(0, 1, 2)

#            -/ПРИМЕЧАНИЕ/-
#            Перекодировкой не обязательно заниматься, т.к. уровни,
#            например, "A","B","C" и так соответсвуют числовым 
#            значениям "1","2","3" и можно с ними все посчитать. 

#            Очистим данные от «NA»:
             x_η2 <- na.omit(x_η2)
             
#            Средние по группам:        
             x_η2.grp.mean <- tapply(x_η2[[2]], x_η2[[1]], FUN = mean) 
             cat("Средние по группам:\n"); print(x_η2.grp.mean); cat("\n")
             
#       2.2) Далее найдём отклонения «групповых средних» от «общего среднего»:
             x_η2.grp.dev <- (x_η2.grp.mean  -  mean(x_η2.grp.mean))
             cat("Отклонения «групповых средних» от «общего среднего»:\n"); print(x_η2.grp.dev); cat("\n")
             
#       2.3) Теперь нам нужно сложить квадраты этих отклонений, посчитав 
#            квадрат отклонения каждого группового среднего столько раз, 
#            сколько в данной группе наблюдений. Другими словами, перед 
#            суммированием каждое значение grp.dev, возведённое в квадрат, 
#            нужно умножить на частоту соответствующего уровня фактора.

             SS_effect <- sum((x_η2.grp.dev^2)*table(x_η2[[1]]))

#            Вычислим общую сумму квадратов отклонений:
             SS_total <- sum((x_η2[[2]] - mean(x_η2.grp.mean))^2)

#            Найдем частное двух сумм:
             η2 <- SS_effect/SS_total
             
#            -/ОПЦИОНАЛЬНО/-
#            Можем найти случйную составляющую, подробнее https://online.stat.psu.edu/stat415/lesson/13/13.2
             SS_residual <- SS_total - SS_effect


# 3) Построим по таблице данных диаграмму размахов.
     plot(x,
          main = main.plot,
          xlab = xlab.plot,
          ylab = ylab.plot,
          col = "papayawhip",)
     points(x_η2.grp.mean,  # - наносим средние значения
            pch = 16,       # - символ точке в виде сплошного кружка
            cex = 2)        # - размер
     
     
     
     
# 4) Интепретация:

#    Таблица РАЗМЕРА ЭФФЕКТА:
#    ------------------------
#    модуль коэффициента	связь
#    <    0.10	не выражена
#    ≥    0.10	слабая
#    ≥    0.30	умеренная
#    ≥    0.50	тесная 


 if  (abs(Ppb) <  0.02)                         {power.com <- "не выражена"}
 if ((abs(Ppb) >= 0.02) & (abs(Ppb) < 0.13))    {power.com <- "слабая"     }
 if ((abs(Ppb) >= 0.13) & (abs(Ppb) < 0.26))    {power.com <- "умеренная"  }
 if  (abs(Ppb) >= 0.26)                         {power.com <- "тесная"     }
 
     
 if  (abs(η2) <  0.02)                         {power1.com <- "не выражена"}
 if ((abs(η2) >= 0.02) & (abs(η2) < 0.13))     {power1.com <- "слабая"     }
 if ((abs(η2) >= 0.13) & (abs(η2) < 0.26))     {power1.com <- "умеренная"  } 
 if  (abs(η2) >= 0.26)                         {power1.com <- "тесная"     }


cat('\nСила связи между переменными:', '\033[1m', names(as.data.frame(x)[1]), '\033[0;0m',
    'и', '\033[1m', names(as.data.frame(x)[2]), '\033[0;0m', ":"                ,"\n")

#    • коэффициент Ppb (коэффициент точечно-бисериальной корреляции Ppb);
#    • коэффициент η2  (коэффициент внутригрупповой корреляции η2).
cat("---------------------------------------------------------------"           ,"\n")
cat("a) \033[34mКоэффициенты:\033[0m"                                           ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾"                                                           ,"\n")
cat("   • коэффициент точечно-бисериальной корреляции Ppb:", Ppb                ,"\n")
cat("   • коэффициент внутригрупповой корреляции      η2 :", η2                 ,"\n")
cat("---------------------------------------------------------------"           ,"\n")
cat("б) \033[34mДополнительные данные:\033[0m"                                  ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                                  ,"\n")
cat("   • общая сумма квадратов       SS_total           :", SS_total           ,"\n")
cat("   • межгруппая сумма квадратов  SS_effect          :", SS_effect          ,"\n")
cat("   • сумма квадратов остатков    SS_residual        :", SS_residual        ,"\n")
cat("---------------------------------------------------------------"           ,"\n")
cat("в) \033[34mДисперсионный анализ встроенной функции:\033[0m"                ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                ,"\n")
M <- lm(x[[2]] ~ x[[1]], data = x);
M <- anova(M); M <- t(M); M <- as.data.frame(M);
M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA) 
names(M) <- c("Factor", "Error", "Total"); M <- t(M)
print(M)
cat("\n   где:\n")
cat("   • DF - степень свободы;           \n")
cat("   • SS - сумма квадратов;           \n")
cat("   • MS - средняя сумма квадратов:   \n")
cat("   • F  - F-статистика;              \n")
cat("   • P  - P-значение.              \n\n")

cat("---------------------------------------------------------------"           ,"\n")
cat("• Вывод: 1) (Ppb) обнаружена ", '\033[1m', power.com ,'\033[0m', "связь", 
    '\x1B[3m', names(as.data.frame(x)[1]), '\x1B[0m',
    "с",          
    '\x1B[3m', names(as.data.frame(x)[2]), '\x1B[0m'                            ,"\n")
                 
                   
cat("•           (η2)  обнаружена ", '\033[1m', power1.com ,'\033[0m', "связь", 
    '\x1B[3m', names(as.data.frame(x)[1]), '\x1B[0m',
    "с",
    '\x1B[3m', names(as.data.frame(x)[2]), '\x1B[0m'                            ,"\n")

cat("---------------------------------------------------------------"           ,"\n")
}



Anova.regr <- function(x, CL=90, inverse=FALSE, main.plot="Диаграмма", xlab.plot = names(x)[1], ylab.plot = names(x)[2])
{
  # inverse - меняет столбцы местами
  
  if (inverse==TRUE){x <- data.frame(x[2], x[1])}
  

  
  
# Регрессионная модель:
# ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾       
# 1) Задача: Определите, насколько данных сильно связаны через:
#    • Введите значение β1;
#    • Введите значение β0;
#    • Определите, насколько хорошо результат
#      одной переменной предсказывает другой, через R2.  
#
#    -/ОБЩАЯ ТЕОРИЯ/-         
#    В рамках регрессионного анализа существует возможность 
#    ПРЕДСКАЗЫВАТЬ значения одной из переменных по значениям другой, поэтому:
#    • переменную-“фактор”    - принято называть «ПРЕДИКТОР» (predictor), 
#    • “зависимую” переменную - принято называть «ОТКЛИК» (response variable).

#    • При ДВУМЕРНЫХ данных модель называют «простой регрессией», в ней:
#      o  один предиктор - “фактор”;         ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
#      o  один отклик    - “зависимая” переменная.

#    • При МНОГОМЕРНЫХ данных модель называют «множественной регрессией», в ней:
#      o  несколько переменых предикторов;     ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾

#    -/ПРИМЕЧАНИЕ/-
#    В отличии от задания с дисперсионным анализом, здесь
#    обе переменные метрические, но мера связи по прежнему
#    будет служить коэффициент  η2 — с точки зрения вычислений 
#    принципиально ничего не изменится, только метод будет называться 
#    уже не дисперсионным, а регрессионным анализом.                   
     
#    2) коэффициент детерминации R²:
#       ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾   
#       Модели «дисперсионного» и «регрессионного» анализа основаны на одном и том же
#       принципе разложения изменчивости зависимой переменной (отклика) на две составляющие:            
#       • эффект, связанный с фактором (предиктором), 
#       • и нормально распределённые остатки. Обе эти модели — разновидности т.н. общей линейной модели    
#           
#       Для вычисления меры связи переменных в модели линейной регрессии мы 
#       можем воспользоваться тем, что в линейной модели общая дисперсия 
#       переменной-отклика равна сумме дисперсий эффекта и остатков:
#       σ²_total = σ²_effect + σ²_residual
#       Соотвественно, мы можем вычислить η2 двумя способами:
#       
#               SS_effect        SS_residual
#       η2  =   —————————  = 1 - ———————————   
#               SS__total         SS__total
#                                 
#       -/ВАЖНО/-
#       • Для «дисперсионного» способа - считать через SS_effect, т.к. есть факторы (есть группы)
#       • Для «регрессионного» способа - считать через SS_residual - сумма квадратов остатков
#       Т.к. в «регрессионном» анализе групп НЕТ, в нем мера связи называется
#       НЕ «коэффициент внутригрупповой корреляции η2», - а «коэффициент детерминации R²»
#                SS_residual
#       R² = 1 - ———————————   
#                 SS__total  
#       -/Примечание/-          
#       В случае ЛИНЕЙНОЙ регрессии коэф. R² можно вычилсть еще R²=p², где p - коэф. линейн. кор. Пирсона.     

#       2.1) Через встроенную функцию найдем суммы квадратов:
               M <- lm(x[[1]] ~ x[[2]], data = x);
               M <- anova(M); M <- t(M); M <- as.data.frame(M);
               M[[3]] <- c(sum(c(M[1,1],c(M[1,2]))), sum(c(M[2,1],c(M[2,2]))), NA, NA, NA) 
               names(M) <- c("Factor", "Error", "Total"); M <- t(M)
               
#       2.2) Получим следующие результаты:
               SS_effect    <- M[4]
               SS_residual <- M[5]
               SS_total    <- M[6]
               R2 <- 1 - (SS_residual/SS_total);  R2
               
#       2.3) Как выше было сказано, R2 можно получить также через лин.коэфф.корреляции p Пирсона:               
               P.Pearson <- function(x.table)
               {
                 x.table <- na.omit(x.table)
                 sd_x    <- sqrt(sum((x.table[[1]] - mean(x.table[[1]]))^2)/nrow(x.table))
                 sd_y    <- sqrt(sum((x.table[[2]] - mean(x.table[[2]]))^2)/nrow(x.table))
                 Σ       <- sum((x.table[[1]]-mean(x.table[[1]]))*(x.table[[2]]-mean(x.table[[2]])))
                 N.σx.σy <- nrow(x.table)*sd_x*sd_y
                 return(Σ/N.σx.σy)
               }
#              Получили тот же самый ответ:
               P <- P.Pearson(x); P 
               R2 <- P^2; R2

#    3) коэффициенты регрессии:
#       ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾     
#       Нужно провести оптимальную линию регрессии через облако рассеяния. 
#       Самый простой способ: линия выбранной формы накладывается на данные 
#       (модель “подгоняется”) так, чтобы остатки (отклонения значений переменной-отклика)
#       от этой линии, оказались минимальными. 
#       Точнее, минимальными должны быть квадраты остатков (сумма этих квадратов) — 
#       такой способ подгонки модели называют «методом наименьших квадратов».
#                                              ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾               
#       По сути дела, построить, или подогнать регрессионную модель: 
#       найти её коэффициенты, β1 и β0 для простой линейной регрессии. 
#              
#                      | Коэффициент β1 определяет угол наклона линии регрессии.
#              σ_Y     | p - коэффициент линейной корреляции Пирсона.      
#       β1 = p ———     |              
#              σ_X     |   
#                            | Коэффициент β0 — свободный член, константа, или интерцепт 
#                            | представляет собой значение Yi при Xi = 0, т.е точку "оси y" 
#       β0 = μ_Y - (β1*μ_X)  | в месте её пересечения (intercept) линией регрессии, фактически           
#                            | величину сдвига всей линии регрессии по оси y относительно параллельной                    
#                            | прямой, проходящей через начало координат. 
#       
#               
#       3.1) Найдем среднеквадратичные отклонения:
             σ_X <- sqrt(sum((x[[1]] - mean(x[[1]]))^2)/length(x[[1]]))
             σ_Y <- sqrt(sum((x[[2]] - mean(x[[2]]))^2)/length(x[[2]])) 
              
#       3.2) Определим угол наклона линии регрессии:
             β1  <- P*(σ_Y/σ_X) 

#       3.3) Определим константу, величину сдвига всей линии регрессии 
#            по оси y относительно параллельной прямой:
             μ_X <- mean(x[[1]])
             μ_Y <- mean(x[[2]])
             
             β0 = μ_Y - (β1*μ_X)

#    4) График:
#       ‾‾‾‾‾‾    
#       4.1) Построим график:
             plot(x,           
                  main = main.plot,
                  xlab = xlab.plot,
                  ylab = ylab.plot,) 
             
#       4.2) Нанесем линию регрессии x_Y на x_X:               
             abline(β0, β1, col = "steel blue")
             
#       4.3) Нанесем линию регрессии x_Y на x_X:               
             β1.yx  <- P*(σ_X/σ_Y) #???
             β1.inverse <- 1/β1.yx  # замена коэффициента регрессии на обратную величину
             β0.inverse <- μ_Y - (β1.inverse*μ_X)  # вычисление свободного члена
             abline(β0.inverse, β1.inverse, col = 'maroon', lty = 'dotted')  # пунктир
             
#       4.4) Нанесем линию регрессии x_Y на x_X:    
             β1.inverse <- 1/β1  
             β0.inverse <- μ_Y - (β1.inverse*μ_X)
             abline(β0.inverse, β1.inverse, col = 'green3', lty = 'dotted')  # пунктир
             
             
             
#       4.5) График с доверитльной полосой: 
             library(ggplot2)

             theme_set(theme_bw(base_size = 14))  # чёрно-белое оформление, базовый размер шрифта
             theme_update(plot.title = element_text(face = 'bold', hjust = 0.5))  # жирный шрифт заголовка, выравнивание по центру
             
             x.gg <-ggplot(x) +
               aes(y = x[[2]], x = x[[1]]) +
               geom_point(col = 'blue', cex = 1.5) +
               geom_smooth(method = 'lm', col = 'red', level = CL/100, fill = 'pink')+
               labs(title = main.plot, 
                    y = ylab.plot, x = xlab.plot)
             print(x.gg)
             
             
             
             
             
             
# 4) Интепретация:
#    ‾‾‾‾‾‾‾‾‾‾‾‾
#    Таблица РАЗМЕРА ЭФФЕКТА:
#    ------------------------
#    модуль коэффициента	связь
#    <    0.10	не выражена
#    ≥    0.10	слабая
#    ≥    0.30	умеренная
#    ≥    0.50	тесная 
if  (abs(R2) <  0.02)                        {power.com <- "не выражена"}
if ((abs(R2) >= 0.02) & (abs(R2) < 0.13))    {power.com <- "слабая"     }
if ((abs(R2) >= 0.13) & (abs(R2) < 0.26))    {power.com <- "умеренная"  }
if  (abs(R2) >= 0.26)                        {power.com <- "тесная"     }

cat('\nМодель регрессии', '\033[1m', names(as.data.frame(x)[2]), '\033[0;0m',
    '«НА»', '\033[1m', names(as.data.frame(x)[1]), '\033[0;0m:'                                                   ,"\n")

#    • коэффициент Ppb (коэффициент точечно-бисериальной корреляции Ppb);
#    • коэффициент η2  (коэффициент внутригрупповой корреляции η2).
cat("----------------------------------------------------------"                                                    ,"\n")
cat("a) \033[34mОценка параметров регрессионной модели:\033[0m"                                                     ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                                                     ,"\n")
cat("   • значение β0, (интерцепт)                :", β0                                                            ,"\n")
cat("   • значение β1, (угол наклона)             :", β1                                                            ,"\n\n")
cat("   • коэффициент детерминации R²             :", R2                                                            ,"\n")
cat("   • стандартная ошибка остатков             :", summary(lm(x[[2]] ~ x[[1]], data = x))[["sigma"]]             ,"\n")
cat("----------------------------------------------------------"                                                    ,"\n")
cat("б) \033[34mСтандартные ошибки для оценок коэффициентов регрессии:\033[0m"                                      ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                                      ,"\n")
cat("   • значение SEb0,                          :", summary(lm(x[[2]] ~ x[[1]], data = x))[["coefficients"]][1,2] ,"\n")
cat("   • значение SEb1,                          :", summary(lm(x[[2]] ~ x[[1]], data = x))[["coefficients"]][2,2] ,"\n")
cat("----------------------------------------------------------"                                                    ,"\n")
cat("в) \033[34mДоверительные интервалы CI при:","\033[0m"                                      ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                                      ,"\n")
for (i in 1:length(CL)){
cat(paste0("При CI=", CL[i],"%:"), "\n")
print(confint(lm(x[[2]] ~ x[[1]], data = x), level = CL[i]/100))
cat("\n")
}
cat("   где:\n")
cat("   • Intercept - b0;\n")
cat("   • x[[n]]    - b1;\n\n")
cat("----------------------------------------------------------"                                                    ,"\n")
cat("г) \033[34mФункция для оценивания параметров регрессионной модели:\033[0m"                                                    ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                                                    ,"\n")
print(summary(lm(x[[2]] ~ x[[1]], data = x)))
cat("\n   где:\n")
cat("   • Estimate Std -  b0 и b1;\n")
cat("   • Std. Error   - SEb0 и SEb1;\n")
cat("   • Residual standard error - стандартная ошибку остатков.\n")
cat("   • Multiple R-squared      - Коэффициент детерминации R²;\n\n")
cat("----------------------------------------------------------"                                                    ,"\n")
cat("д) \033[34mДополнительные данные:\033[0m"                                                                      ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                                                                      ,"\n")
cat("   • общая сумма квадратов       SS_total    :", SS_total                                                      ,"\n")
cat("   • межгруппая сумма квадратов  SS_effect   :", SS_effect                                                     ,"\n")
cat("   • сумма квадратов остатков    SS_residual :", SS_residual                                                   ,"\n")
cat("----------------------------------------------------------"                                                    ,"\n")
cat("е) \033[34mРегрессионный анализ встроенной функции:\033[0m"                                                    ,"\n")
cat("   ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾"                                                                    ,"\n")
print(M)
cat("\n   где:\n")
cat("   • DF - степень свободы;\n")
cat("   • SS - сумма квадратов;\n")
cat("   • MS - средняя сумма квадратов:\n")
cat("   • F  - F-статистика;\n")
cat("   • P  - P-значение.\n\n")

cat("----------------------------------------------------------"           ,"\n")
cat("• Вывод: обнаружена ", '\033[1m', power.com ,'\033[0m', "связь", 
    '\x1B[3m', names(as.data.frame(x)[2]), '\x1B[0m',
    "с",
    '\x1B[3m', names(as.data.frame(x)[1]), '\x1B[0m'                            ,"\n")
cat("-----------------------------------------------------------"          ,"\n")

}









Static_output <- function(x, CL_n=95, CL_n_group=NULL, p_SEP=NULL, p_SEP.onli=NULL, CI_coeff=FALSE){
  
  # CL_n - коэффициент доверия
  # CL_n_group - коэф. доверия для групп (если оидн из столцов содержит слова и его можно разбирть на группы)
  # p_SEP - доля %, считает не более этого числа
  # CI_coeff - если нужны доверительыне интервалы при коэффициентах корреляции
  
  if (!is.null(p_SEP.onli)){
    cat("\n")
    
    p <- p_SEP.onli[1]
    n <- p_SEP.onli[2]
    
    σ2 <- p*(1-p)
    SEp <- sqrt(σ2/n);  SEp
    cat("SEp:", (SEp),    "- cтандартная ошибка выборочной доли;","\n")
    
    for (i in 1:length(CL_n)){
      CL <- CL_n/100
      a <- 1-CL[i]
      z <- qnorm(0.5*a)
      z <- (0-z)
      MOE <- z * SEp;  MOE
      cat("MOE:", (MOE),    "- cтатистическая погрешность выборочной доли при CL:", CL_n[i],     "\n")
    }
    cat("\n")
    break
  }

  
  x <- na.omit(x)
  
  if (length(as.data.frame(x)) > 1) {
  
  if ((is.character(x[[1]])) | (is.character(x[[2]]))) {
  if (!is.character(x[[2]])){
    x <- x[c(2,1)]
  }
  }
  else  if ((is.factor(x[[1]])) | (is.factor(x[[2]]))) {
    if (!is.factor(x[[2]])){
      x <- x[c(2,1)]
    }
  } 
    
    
  }


# 1) -/ТЕОРИЯ/-
#    ------------------ 
#    СТАТИЧЕСКИЙ ВЫВОД:
#    ------------------    
#    «НАДЁЖНОСТЬ», или «ТОЧНОСТЬ» статистического вывода зависит от того, 
#    насколько выборка репрезентативна, то есть адекватно представляет генеральную совокупность.     
#                                               ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯        
#    Наверняка этого знать НЕ можем, но теоретически ожидаемая репрезентативность максимальна, 
#    если все наблюдения генеральной совокупности имеют РАВНУЮ вероятность попасть в выборку
#    — такая выборка называется случайной (random sample). 
#                               ¯¯¯¯¯¯¯¯¯   
#    Т.е.
#    
#    • Весь частотный статистический вывод строится на допущении о случайности выборки.
#                     ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯                ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   
#    • Надёжность вывода зависит и от объёма выборки.
#      ¯¯¯¯¯¯¯¯¯¯                     ¯¯¯¯¯¯¯¯¯¯¯¯¯¯
#    
#    ----------------------------           
#    Школы статистического вывода     
#    ---------------------------- 
#    • «частотный», или классический подход строится на статистическом, или объективном определении вероятности 
#       как относительной частоты события при бесконечном числе наблюдений.   
#        
#    • «Байесовский» подход использует субъективное определение: вероятность —
#       это степень нашей уверенности в неизвестном нам исходе события, потенциального либо уже произошедшего. 
#    
#    В ДАННОМ ЗАДАЧЕ «ЧАСТОТНЫЙ ПОДХОД»! 
# 
#    
# 2) Напишем функцию, которая выдаст таблицу со всеми ответами:  
     x.Stat <- function(vectot, var=NULL){
       a<-1
#     
#    В статистическом выводе анализируем изменчивость признаков при частотном подходе 
#    имея дело с двумя распределениями одной и той же переменной: 
#    - c распределением генеральной совокупности и выборки.
#        • Сводные характеристики распределения генеральной совокупности называются параметрами; 
#        • аналогичные выборочные характеристики — статистиками.
#    
#    • Генеральная совокупность (population) - множество всех потенциальных наблюдений изучаемого явления 
#    • Выборка (sample) - имеющиеся у нас данные — это наблюдения, выборочно полученные из генеральной совокупности.
#    
#    
#          • Параметры - то, что хотим оценить (не можем их вычилсить, но в районе какого значения указать);
#          • Статистики - то, что можем вычислить;                    
#    
#    2.2) Характеристика (Сводные характеристики распределения генеральной совокупности):  
#    
          μ  <- mean(vectot[[a]]); μ
#    
#         • Несмещённость (unbiasedness) оценки — это свойство её математического ожидания 
#             (теоретического среднего) совпадать с истинным значением параметра.
#           
#           Выборочная доля градации переменной — это несмещённое (unbiased) средство оценивания её доли в генеральной совокупности. 
#           Несмещёнными оценками соответствующих параметров служат также выборочное среднее и некоторые другие статистики.
#                        
            σ2 <- sum((vectot[[a]] - mean(vectot[[a]]))^2)/length(vectot[[a]]); σ2
            σ  <- sqrt(σ2); σ
            
            

#    2.3) Статистика:
#         
#         Нас интересует распределение генеральной совокупности и его параметры, 
#         но можем найти только их приблизительные значения.
#         (приближённые вычисления называются оцениванием (estimation)).
#                   
          x  <- sum(vectot[[a]])/(length(vectot[[a]])); vectot
          s2 <- var(vectot[[a]]); s2
          s  <- sd(vectot[[a]]); s
#              
#    2.4) Функция R
          if (μ == x ) {R.μ.x   <-"совпадают"}
          if (σ2== s2) {R.σ2.s2 <-"совпадают"} 
          if (σ == s ) {R.σ.s   <-"совпадают"} 
          
          if (μ  < x ) {R.μ.x   <-"выборочное"} 
          if (σ2 < s2) {R.σ2.s2 <-"выборочное"} 
          if (σ  < s ) {R.σ.s   <-"выборочное"} 
          
          if (μ  > x ) {R.μ.x   <-"генеральное"} 
          if (σ2 > s2) {R.σ2.s2 <-"генеральное"} 
          if (σ  > s ) {R.σ.s   <-"генеральное"}                  
          
          x.DF <- data.frame("1" = c("μ :", "σ²:", "σ :"),
                             "характеристика"= c(μ, σ2, σ),
                             "1" = c("x :", "s²:", "s :"),
                             "статистика"    = c(x, s2, s),
                             "Функция R"     = c(R.μ.x, R.σ2.s2, R.σ.s))
          names(x.DF) <- c("", "характеристика", "    ", "статистика", "   Функция R")
          
          if (is.null(var)) {return(x.DF)}
          if (var=="μ")     {return(μ)}
          if (var=="σ2")    {return(σ2)}
          if (var=="σ")     {return(σ)}
          if (var=="x")     {return(x)}
          if (var=="s2")    {return(s2)}
          if (var=="s")     {return(s)}}

#     Ответ:
      cat("Для переменной:", names(x[1]), "\n"); print(x.Stat(x)); cat("\n")

# 3) Оцените по этой выборке стандартные ошибки:  

#    При статистическом оценивании выяснить, насколько полученная оценка ТОЧНА.
#    Точность оценки выражается через её РАСХОЖДЕНИЕ с истинным значением параметра.        
#    Оно нам неизвестно, поэтому выяснить, насколько точна наша выборочная оценка, мы не можем, 
#    но зато можем определить, насколько расходятся такие выборочные оценки в целом, каков их разброс.          
#                              ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯              
#       В качестве меры разброса используется «стандартное отклонение». 
#       Теоретически для его вычисления должны извлечь из генеральной совокупности бесконечное число 
#       случайных выборок и по каждой из них оценить параметр, т.е. найти для каждой выборки соответствующую статистику. 
#       Так мы получим множество значений статистики, которое имеет своё распределение — выборочное распределение (sampling distribution).        
#                                                                                        ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯                 
#    Если статистика является несмещённым средством оценивания, то 
#    • центр её выборочного распределения и есть истинное значение параметра, 
#    • а отклонение конкретной оценки от центра интерпретируется как величина её неточности, ошибки.
#      
#    Стандартное отклонение выборочного распределения называется стандартной ошибкой SE.
#    
#    Формулы SE описывает насколько точно получается оценить неизвестный параметр по выборочному значению (по статистике).
#    
#    3.1) Cтандартная ошибка выборочного среднего SEM:
#         ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
#         Стандартная ошибка среднего, SEM — это максимальная величина, на которую оценка посредством выборочного среднего 
#         отклоняется от значения среднего данной генеральной совокупности примерно в 2/3 выборок такого же объёма.
#    
#         Стандартная ошибка среднего, SEM - это максимальная величина, в пределах которой отклоняется от генерального среднего 2/3 выборочных средних.
#         т.е. берем много выборок и считаем много средних и смотрим как средние распределяются.
#         И, оказывается, 2/3 средних будут лежать в интервале от "-" до "+" 1 стандартная ошибка от генерального среднего.
#    
#         Т.е. SEM - стандартное отклонение среднего, которое принято называть стандартной ошибкой, 
#         потому что, когда оцениваем, интепретируем отклонение нашей оценки как её ошибку. Чем больше октлоняется, тем сильнее ошибаемся.                     
#    
#         Т.е. если будем таким способом оценивать, то в 2/3 случаев наш результат +- 1 стандартное отклоение от генерального отклонения.                          
                                  
          n  <- length(x[[1]])
          σ  <- x.Stat(x, "σ")
          s  <- x.Stat(x, "s")
          SEM_σ <- σ/sqrt(n);  SEM_σ
            
#         Формула дает точное значение стандартной ошибки, но 
#         недостаток: σ — это генеральное стандартное отклонение.

#         Мы не можем пользоваться этой формулой, т.к.СИГМА не известна - это параметр. Мы не можем использвать их для вычислений.
#         • Параметры - то, что хотим оценить (не можем их вычилсить, но в районе какого значения указать);
#         • Статистики - то, что можем вычислить;    


#         Вместо него использовать его несмещённую оценку 
#         — исправленное стандартное отклонение выборки:
          SEM_s <- s/sqrt(n);  
          cat("SEM:", round(SEM_s, 6),  "- стандартная ошибка выборочного среднего;",          "\n")

          
#         -/ПРИМЕЧАНИЕ/-   
#         Поскольку в последнюю формулу входит не σ, а её оценка, то и 
#         практически вычисляемая стандартная ошибка — это не истинное значение параметра, 
#         а его оценка, статистика. Глядя на формулу, мы также замечаем, что 
#         чем меньше разброс наблюдений переменной и больше объём выборки, тем меньше SEM, 
#         т.е. выше точность оценки среднего.
          

#    3.2) Доля var1, чья var2 не превышает "p_SEP"%:
#         ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯  
#         p - доля (proportion) 
          
          if (!is.null(p_SEP)){
          p <- pnorm(p_SEP+1, mean = mean(x[[1]]), sd = x.Stat(x, "σ"))

#    3.3) Стандартная ошибку такой выборочной доли SEp:
#         ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
#         При оценивании доли p (proportion) мы можем перекодировать данные как бинарные, 
#         по признаку наличия либо отсутствия “успеха,” т.е. одной из градаций. 
#         В таком случае долю можно рассматривать как среднее нулей и единиц, 
#         и, если выборка не слишком маленькая, то его выборочное распределение оказывается близким к нормальному.
#         
#         SS = pn × (1-p)² + (1-p)n × (0-p)²
#          
#              pn(1-p)² + (1-p)np²   pn(1-p)((1-p)+p)
#         σ² = ——————————————————— = ———————————————— = p(1-p)
#                       n                   n
          SS <- (p*n) * (1-p)^2 + ((1-p)*n * (0-p)^2)
          σ2 <- p*(1-p)

          SEp <- sqrt(σ2/n);  SEp
          
          cat("SEp:", round(SEp, 6),    "- cтандартная ошибка выборочной доли;",               "\n")
#                
#         Интерпретируется SEp аналогично SEM, как наибольшее отклонение выборочной 
#         доли от доли генеральной совокупности примерно в 2/3 выборок такого же объёма.
#          
#          
#         Как интепретировать стандартную ошибку? Есть проблема, 
#         есть параметр, который хотим оценить и у него не может быть ошибки,
#         это константа.
#          
#         У нас есть наши данные, по которым посчитали статистику - являются константами, 
#         они никуда случайно изенятся не могут. Как можем представить себе, что там может случайно меняться?
#         Мы будем набирать много выборок из генеральной совокупности и по каждой выборке оценивать параметры.  


# 4) Найдите для такой выборочной доли статистическую погрешность MOE с CL = CL_n%:
#                      ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
#    При интепретации SEM говорили, что если будем таким способом оценивать, то в 2/3 случаев наш результат +- 1 стандартное отклоение от генерального отклонения. 2/3 выборочных средних попадает.                          
#    Это не очень удобно и понятно, поэтому возьмем другой интервал. Это уже не стандартная ошибка, а статистическая погрешность.
#    Она аналогична по смыслу, тоже есть граничные значения, за пределых которых не выходят определнное число наблюдений,
#    только не 68%, а можно задать любую долю (обычно 90-95%)

#    MOE = (o - z) × SE    
          for (i in 1:length(CL)){
#    4.1) Находим для заданного уровня доверия CL вероятность ⍺: ⍺=1-CL
          CL <- CL_n/100
          a <- 1-CL[i]

#    4.2) Находим квантиль стандартного нормального распределения 0.5⍺ и отнимем от 0,
#         т.е. меняем знак на положительный     
          z <- qnorm(0.5*a)
          z <- (0-z)
#    4.3) Переводим найденную величину в значение исходной шкалы, умножим её на SE.
#         Проученный результат - погрешность с уровнем доверимем ⍺.
          MOE <- z * SEp;  MOE
          cat("MOE:", round(MOE, 6),    "- cтатистическая погрешность выборочной доли при CL:", CL[i],     "\n")
          }
          cat("\n")
          }
          
                  
# 5) Найдите статистическую погрешность выборочного среднего MOE с CL = СL_n%:
#            ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯          
#    Погрешность для среднего определяется с помощью квантилей стандартного нормального распределения только при условии, что нам известно истичное значение SЕМ. 

#    Для нахождения МОЕ в этом случае используют квантили уже не стандартного нормального распределения, это распределения t-Стьюдента, похожего по форме но с более
#    толстыми хвостами — соответственно, квантиль a/2 в нём расположен дальше от центра. Пропорции распределения меняются в зависимости от его параметра — числа
#    степеней свободы df = n - 1: чем больше выборка, тем оно больше, и тем распределение становится ближе к стандартному нормальному (хвосты становятся тоньше)
       
#    Порядок вычисления МОЕ остается тем же, но на 2-м шаге вместо нормального распределения используем также симметричное распределение t-Стьюдента со степенями свободы df = n - 1.     
     CL <- CL_n/100
     
     for (i in 1:length(CL)){
     a <- 1-CL[i]
     z <- qt(0.5*a, n-1)
     z <- (0-z)
     MOE_sem <- z * SEM_s 
     cat("MOE:", round(MOE_sem, 6),"- статистическая погрешность выборочного среднего;","\n")
     }
     cat("\n")
     
# 6) доверительный интервал CI
#    ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
#    Одна из идей интервального оценивания состоит в том, чтобы привязать погрешность 
#    не к истинному значению параметра, а к конкретной полученной нами точечной оценке этого параметра.
#    
#    В случае симметричного выборочного распределения это легко можно сделать, 
#    отложив по MOE в обе стороны от имеющейся точечной оценки.
#    Это границы интервала - доверительный интервал CI (confidence interval).
#                            ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
#    Формула для 95%:
# 
#    μ - (1.96*SEM) ≤ x ≤ μ + (1.96*SEM), где x - выборочное среднее
#
#     
#    Пример как выглядит на графике:
#                _
#              _ | _
#             _  |  _
#            _   |   _
#           _ |  |  | _ 
#          —  |  |  |  —
#     ___—‾|  |  |  |  |‾—___
#    ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾      
#        -2  -1  0  1  2     σ
#     
#   Диапозон: от -1 до 1 - 68,26%
#             от -2 до 2 - 95,44%
#   
#   Т.е. уровень доверия - какой процент попадает в эти границы.
#
#   В "формуле" μ - (1.96*SEM) ≤ x ≤ μ + (1.96*SEM):
#   
#     1.96: 95.00%  доверительный интверал;
#     1.00: 68.26%  доверительный интверал и т.д. 
#    
#   Посчитаем:
    μ  <- x.Stat(x, "μ")
    X  <- x.Stat(x, "x")
    
#   Пример расчета:    
#   (μ - (MOE_sem),"≤",X ,"≤", μ + (MOE_sem)
    
    for (i in 1:length(CL)){
      a <- 1-CL[i]
      z <- qt(0.5*a, n-1)
      z <- (0-z)
      MOE_sem <- z * SEM_s 
      cat("Доверительный интервал CI при CL=", paste0(CL_n,"%:"), μ - (MOE_sem),"≤",X ,"≤", μ + (MOE_sem), "\n")
    }
    cat("\n")
    
#    интерпретация доверительного интервала:
#    ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
#    Доверительный интервал CI с уровнем доверия CL — средство оценивания параметра в виде диапазона значений, 
#    который с вероятностью, равной CL, будет содержать истинное значение, накроет (cover) его.
# 

     if (CI_coeff==TRUE){
# 7) доверительный интервал CI коэффициентов корреляции
#    ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ 
#     7.1) Выборочное распределение коэффициента линейной корреляции Пирсона асимметрично при любых его значениях, отличных от нуля. 
#          Если принять допущение о нормальности двумерного распределения (облако рассеивания имеет форму эллипса), 
#          то для вычисления доверительного интервала значение выборочного коэффициента r можно преобразовать так, 
#          чтобы его распределение стало близким к стандартному нормальному:
           z_ρ  <- atanh(cor(x, method = "pearson") [2]) 
           z_rs <- atanh(cor(x, method = "spearman")[2])

#          Это т.н. z-преобразование Фишера, состоящее в нахождении ареатангенса (обратного гиперболического тангенса) r.

#     7.2) Для вычисления статистической погрешности нам требуется найти стандартную ошибку коэффициента, также преобразованную в z-значение. 
#          Для этого достаточно выполнить первый этап вычисления стандартной ошибки SEr, на котором она находится как SEz, т.е. именно в единицах Z-шкалы.
           
           cat("Стандартные ошибки для коэффициентов корреляции:\n")
           
#          7.2.1) Пирсона:
           
#                 Формула:         Выборочное распределение коэффициента линейной корреляции Пирсона далеко от 
#                          1       нормального и меняет форму в зависимости от величины коэффициента, поэтому 
#                 SEz = −−−−−−     вычислить его стандартную ошибку несколько сложнее. Она находится в два этапа:
#                       √(n-3)     сначала стандартная ошибка вычисляется в виде z-значения.
             
                  SEz_ρ <-1/sqrt(nrow(x)-3)
             
#                 Затем, чтобы вернуться из Z-шкалы в шкалу значений коэффициента корреляции, необходимо выполнить т.е.
#                 обратное z-преобразование Фишера, т.е. найти гиперболический тангенс SEz:  
                  
                  SEr_ρ <- tanh(SEz_ρ)
                  
                  cat("• Пирсона  ρ: ",  SEr_ρ, "\n")
                  
#          7.2.2) Спирмена:                  
           
                  SEz_rs <- sqrt((1+rs^2/2)/(nrow(x)-3))
                  SEr_rs <- tanh(SEz_rs)
                  cat("• Спирмена rs:",  SEr_rs, "\n\n")
                  

#     7.3) Далее строим доверительный интервал тем же способом, что для доли и среднего:
#          помощью SEz находим по стандартному нормальному распределению статистическую погрешность MOE для заданного уровня доверия 
#          и откладываем её влево и вправо от z-значения коэффициента корреляции (соответственно вычитаем из него и прибавляем к нему).        

           cat("Доверительные интервалы CI для коэффициентов корреляции:\n")      
                  
           for (i in 1:length(CL)){
             # CL[i] <- CL[i]/100
             alpha <- (1 - CL[i])/2
             quantile <- qnorm(alpha)
             MOE_ρ <- quantile * SEz_ρ
             MOE_rs<- quantile * SEz_rs
             
#          Чтобы получить доверительный интервал для r, остаётся преобразовать оба 
#          найденные z-значения границ CI в значения исходной шкалы коэффициента 
#          корреляции тем же преобразованием, которое использовалось при вычислении 
#          стандартной ошибки, т.е. обратным z-преобразованием Фишера. Доверительный 
#          интервал для коэффициента ранговой корреляции rs Спирмена строится тем же 
#          способом, но со своей формулой для стандартной ошибки.
             
           CImin_ρ  <- tanh(z_ρ + MOE_ρ) # откладываем её влево и вправо от z-значения коэффициента корреляции 
           CImax_ρ  <- tanh(z_ρ - MOE_ρ)
           
           CImin_rs <- tanh(z_rs + MOE_rs) # откладываем её влево и вправо от z-значения коэффициента корреляции 
           CImax_rs <- tanh(z_rs - MOE_rs)
           
           cat("• Пирсона:  при CL=", paste0(CL[i]*100,"%:"), (CImin_ρ)," − ", (CImax_ρ), "\n")
           cat("• Спирмена: при CL=", paste0(CL[i]*100,"%:"), (CImin_rs)," − ",(CImax_rs), "\n")
           
           CI.DF_cor <- data.frame("Метод" =c("• Пирсона: ", "• Спирмена:"),
                                   "при CI"=c(paste0(CL[i]*100,"%:"), paste0(CL[i]*100,"%:")),
                                   "CI min"=c(CImin_ρ, CImin_rs),
                                   "CI max"=c(CImax_ρ, CImax_rs)
                                   )
           }; print(CI.DF_cor) 
     }           
     
# 8) Расчет доверительного интервала для групп:  
     
     if (!is.null(CL_n_group)) { 
       CL_group <- CL_n_group/100
       a1 <- 1-CL_group
       
       x.grp.mean   <- tapply(x[[1]], x[[2]], FUN = mean)  
       x.grp.sd   <- tapply(x[[1]], x[[2]], FUN = sd) 
       
       n_group <- length(x.grp.sd)
       n <- nrow(x)/n_group
       
       SEM_vec <- 0
       MOE_sem_vec<- 0
       
#    μ - MOE ≤ X ≤ μ + MOE, где x - выборочное среднее
       
       CI.DF<- data.frame("var1")
       for (i in 1:n_group) {
         SEM_vec[i] <- x.grp.sd[[i]]/sqrt(n)
         MOE_sem_vec[i] <- (0 - qt(0.5*a1, n-1)) * SEM_vec[i]
   
         CI.DF[i,1]<-names(x.grp.sd[i])
         
         CI.DF[i,2]<-x.grp.mean[i] - (MOE_sem_vec[i])
         CI.DF[i,3]<-"≤"
         CI.DF[i,4]<-x.grp.mean[i]
         CI.DF[i,5]<-"≤"
         CI.DF[i,6]<-x.grp.mean[i] + (MOE_sem_vec[i])
         CI.DF[i,7]<-" "
         CI.DF[i,8]<-MOE_sem_vec[i]
         CI.DF[i,9]<-SEM_vec[i] 
         
         
         i<-i+1
         }
       
       cat("Доверительный интервал CI для разных групп при CL =", paste0(CL_n_group,"%:"), "\n\n")  
       names(CI.DF) <- c("группа","μ - MOE", "  ", "   X   ", "  ", "μ + MOE", "", "MOE", "SEM")
       print(CI.DF)
       cat("\n\n")
         
       CI.DF2 <- CI.DF
       CI.DF2 <- CI.DF2[, -3]
       CI.DF2 <- CI.DF2[, -4]
       CI.DF2 <- CI.DF2[, -5]
       CI.DF2 <- CI.DF2[, -5]
       CI.DF2 <- CI.DF2[, -5]
       return(CI.DF2)
     }
}